{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "024a4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# import torch as T\n",
    "# import os\n",
    "\n",
    "# pth = \"./dataset_embeddings/convnextv2_atto.fcmae_ft_in1k\"\n",
    "# num_encoding_blocks = len(os.listdir(pth))\n",
    "\n",
    "# embeddings = T.zeros(num_encoding_blocks * 250, 320, 7, 7)\n",
    "# valid = T.zeros(num_encoding_blocks * 250, dtype=T.bool)\n",
    "\n",
    "# print(f\"number of encodings: {len(os.listdir(pth))}\")\n",
    "# for (i, f) in tqdm.tqdm(enumerate(os.listdir(pth)), total=num_encoding_blocks, desc=\"Loading encodings\"):\n",
    "#     ten = T.load(os.path.join(pth, f), map_location=\"cpu\")\n",
    "#     block_size = ten.size(0)\n",
    "#     embeddings[(i * 250):(i * 250 + block_size), ...] = ten[:]\n",
    "#     valid[(i * 250):(i * 250 + block_size)] = True\n",
    "\n",
    "# T.save(embeddings, \"./dataset_embeddings/convnextv2_atto.fcmae_ft_in1k-embeddings.pt\")\n",
    "# T.save(valid,      \"./dataset_embeddings/convnextv2_atto.fcmae_ft_in1k-valid.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e35d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noone\\Documents\\programming\\motion-capture\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "from motion_capture.model import modules\n",
    "\n",
    "model = modules.VisionModule(\n",
    "    backbone = \"convnextv2_atto.fcmae_ft_in1k\",\n",
    "    heads = {\n",
    "        \"personBbox\": {\n",
    "            \"input_dim\": 320,\n",
    "            \"input_sequence_length\": 49,\n",
    "            \n",
    "            \"output_dim\": 4,\n",
    "            \"output_sequence_length\": 10,\n",
    "            \n",
    "            \"depth\": 1,\n",
    "            \"continuous_output\": True,\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bfba276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speedtest engaged for <class 'motion_capture.model.modules.VisionModule'>\n",
      "\t# parameters: 9.483732 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "speedtest: 100%|██████████| 1000/1000 [00:09<00:00, 109.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfps: 109.11860061410867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from motion_capture.core.benchmark import model_speedtest\n",
    "\n",
    "model_speedtest(model, (1, 3, 224, 224))\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8776ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone = model.backbone.eval()\n",
    "model.heads = model.heads.train()\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "x, y = T.randn(1, 3, 224, 224).to(\"cuda\"), T.randn(1, 10, 4).to(\"cuda\")\n",
    "opt = T.optim.SGD(model.heads.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82efa73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3749860525131226\n",
      "loss: 1.3730440139770508\n",
      "loss: 1.3708542585372925\n",
      "loss: 1.3682247400283813\n",
      "loss: 1.3648370504379272\n",
      "loss: 1.3601421117782593\n",
      "loss: 1.3532341718673706\n",
      "loss: 1.3429816961288452\n",
      "loss: 1.3293185234069824\n",
      "loss: 1.3151813745498657\n",
      "loss: 1.3039880990982056\n",
      "loss: 1.295972228050232\n",
      "loss: 1.2900131940841675\n",
      "loss: 1.2852712869644165\n",
      "loss: 1.281272292137146\n",
      "loss: 1.2777514457702637\n",
      "loss: 1.274553656578064\n",
      "loss: 1.2715834379196167\n",
      "loss: 1.2687782049179077\n",
      "loss: 1.2660959959030151\n",
      "loss: 1.2635071277618408\n",
      "loss: 1.260991096496582\n",
      "loss: 1.2585315704345703\n",
      "loss: 1.2561168670654297\n",
      "loss: 1.2537380456924438\n",
      "loss: 1.2513874769210815\n",
      "loss: 1.249060034751892\n",
      "loss: 1.2467507123947144\n",
      "loss: 1.2444559335708618\n",
      "loss: 1.2421725988388062\n",
      "loss: 1.2398979663848877\n",
      "loss: 1.2376303672790527\n",
      "loss: 1.2353676557540894\n",
      "loss: 1.2331081628799438\n",
      "loss: 1.2308508157730103\n",
      "loss: 1.228594183921814\n",
      "loss: 1.2263375520706177\n",
      "loss: 1.224079966545105\n",
      "loss: 1.2218207120895386\n",
      "loss: 1.2195594310760498\n",
      "loss: 1.2172952890396118\n",
      "loss: 1.2150284051895142\n",
      "loss: 1.212758183479309\n",
      "loss: 1.210484504699707\n",
      "loss: 1.208207368850708\n",
      "loss: 1.2059268951416016\n",
      "loss: 1.2036432027816772\n",
      "loss: 1.2013567686080933\n",
      "loss: 1.1990675926208496\n",
      "loss: 1.196776270866394\n",
      "loss: 1.1944833993911743\n",
      "loss: 1.1921894550323486\n",
      "loss: 1.189895510673523\n",
      "loss: 1.187601923942566\n",
      "loss: 1.1853100061416626\n",
      "loss: 1.1830201148986816\n",
      "loss: 1.1807339191436768\n",
      "loss: 1.1784518957138062\n",
      "loss: 1.1761751174926758\n",
      "loss: 1.1739052534103394\n",
      "loss: 1.1716430187225342\n",
      "loss: 1.1693896055221558\n",
      "loss: 1.1671464443206787\n",
      "loss: 1.1649143695831299\n",
      "loss: 1.1626945734024048\n",
      "loss: 1.160488486289978\n",
      "loss: 1.1582969427108765\n",
      "loss: 1.1561216115951538\n",
      "loss: 1.1539626121520996\n",
      "loss: 1.1518216133117676\n",
      "loss: 1.1496996879577637\n",
      "loss: 1.147597312927246\n",
      "loss: 1.1455157995224\n",
      "loss: 1.1434557437896729\n",
      "loss: 1.1414183378219604\n",
      "loss: 1.1394037008285522\n",
      "loss: 1.1374130249023438\n",
      "loss: 1.135446548461914\n",
      "loss: 1.1335052251815796\n",
      "loss: 1.1315895318984985\n",
      "loss: 1.12969970703125\n",
      "loss: 1.1278361082077026\n",
      "loss: 1.1259993314743042\n",
      "loss: 1.1241892576217651\n",
      "loss: 1.1224063634872437\n",
      "loss: 1.120651125907898\n",
      "loss: 1.1189230680465698\n",
      "loss: 1.1172224283218384\n",
      "loss: 1.1155493259429932\n",
      "loss: 1.1139037609100342\n",
      "loss: 1.1122854948043823\n",
      "loss: 1.1106942892074585\n",
      "loss: 1.1091302633285522\n",
      "loss: 1.1075931787490845\n",
      "loss: 1.1060824394226074\n",
      "loss: 1.1045984029769897\n",
      "loss: 1.1031402349472046\n",
      "loss: 1.1017076969146729\n",
      "loss: 1.100300669670105\n",
      "loss: 1.0989185571670532\n",
      "loss: 1.0975608825683594\n",
      "loss: 1.0962275266647339\n",
      "loss: 1.094918131828308\n",
      "loss: 1.0936319828033447\n",
      "loss: 1.092368721961975\n",
      "loss: 1.0911277532577515\n",
      "loss: 1.0899091958999634\n",
      "loss: 1.0887119770050049\n",
      "loss: 1.087536096572876\n",
      "loss: 1.0863806009292603\n",
      "loss: 1.0852457284927368\n",
      "loss: 1.0841301679611206\n",
      "loss: 1.0830341577529907\n",
      "loss: 1.0819571018218994\n",
      "loss: 1.0808981657028198\n",
      "loss: 1.0798574686050415\n",
      "loss: 1.0788342952728271\n",
      "loss: 1.077828288078308\n",
      "loss: 1.076838731765747\n",
      "loss: 1.075865626335144\n",
      "loss: 1.0749083757400513\n",
      "loss: 1.0739665031433105\n",
      "loss: 1.0730398893356323\n",
      "loss: 1.0721279382705688\n",
      "loss: 1.071230173110962\n",
      "loss: 1.0703463554382324\n",
      "loss: 1.0694762468338013\n",
      "loss: 1.0686193704605103\n",
      "loss: 1.0677752494812012\n",
      "loss: 1.0669440031051636\n",
      "loss: 1.0661247968673706\n",
      "loss: 1.0653173923492432\n",
      "loss: 1.0645216703414917\n",
      "loss: 1.0637372732162476\n",
      "loss: 1.062963843345642\n",
      "loss: 1.0622011423110962\n",
      "loss: 1.0614489316940308\n",
      "loss: 1.0607068538665771\n",
      "loss: 1.0599746704101562\n",
      "loss: 1.059252142906189\n",
      "loss: 1.0585391521453857\n",
      "loss: 1.0578351020812988\n",
      "loss: 1.0571399927139282\n",
      "loss: 1.0564539432525635\n",
      "loss: 1.0557760000228882\n",
      "loss: 1.0551064014434814\n",
      "loss: 1.0544447898864746\n",
      "loss: 1.0537910461425781\n",
      "loss: 1.0531450510025024\n",
      "loss: 1.0525063276290894\n",
      "loss: 1.0518749952316284\n",
      "loss: 1.0512508153915405\n",
      "loss: 1.0506333112716675\n",
      "loss: 1.0500226020812988\n",
      "loss: 1.0494184494018555\n",
      "loss: 1.0488207340240479\n",
      "loss: 1.0482292175292969\n",
      "loss: 1.0476438999176025\n",
      "loss: 1.0470643043518066\n",
      "loss: 1.0464905500411987\n",
      "loss: 1.0459226369857788\n",
      "loss: 1.0453600883483887\n",
      "loss: 1.0448029041290283\n",
      "loss: 1.0442508459091187\n",
      "loss: 1.0437040328979492\n",
      "loss: 1.043162226676941\n",
      "loss: 1.0426253080368042\n",
      "loss: 1.0420929193496704\n",
      "loss: 1.0415652990341187\n",
      "loss: 1.0410422086715698\n",
      "loss: 1.0405235290527344\n",
      "loss: 1.0400091409683228\n",
      "loss: 1.0394991636276245\n",
      "loss: 1.0389928817749023\n",
      "loss: 1.038490891456604\n",
      "loss: 1.0379928350448608\n",
      "loss: 1.0374984741210938\n",
      "loss: 1.0370079278945923\n",
      "loss: 1.0365209579467773\n",
      "loss: 1.0360376834869385\n",
      "loss: 1.035557746887207\n",
      "loss: 1.035081148147583\n",
      "loss: 1.0346081256866455\n",
      "loss: 1.0341380834579468\n",
      "loss: 1.0336713790893555\n",
      "loss: 1.0332077741622925\n",
      "loss: 1.0327470302581787\n",
      "loss: 1.0322892665863037\n",
      "loss: 1.0318344831466675\n",
      "loss: 1.031382441520691\n",
      "loss: 1.030933141708374\n",
      "loss: 1.0304864645004272\n",
      "loss: 1.0300425291061401\n",
      "loss: 1.0296010971069336\n",
      "loss: 1.0291622877120972\n",
      "loss: 1.0287257432937622\n",
      "loss: 1.0282915830612183\n",
      "loss: 1.0278599262237549\n",
      "loss: 1.0274304151535034\n",
      "loss: 1.027003288269043\n",
      "loss: 1.0265783071517944\n",
      "loss: 1.0261553525924683\n",
      "loss: 1.0257346630096436\n",
      "loss: 1.0253158807754517\n",
      "loss: 1.0248991250991821\n",
      "loss: 1.0244842767715454\n",
      "loss: 1.0240713357925415\n",
      "loss: 1.0236603021621704\n",
      "loss: 1.0232511758804321\n",
      "loss: 1.0228437185287476\n",
      "loss: 1.0224380493164062\n",
      "loss: 1.0220340490341187\n",
      "loss: 1.0216318368911743\n",
      "loss: 1.0212310552597046\n",
      "loss: 1.0208319425582886\n",
      "loss: 1.0204344987869263\n",
      "loss: 1.0200386047363281\n",
      "loss: 1.0196441411972046\n",
      "loss: 1.0192512273788452\n",
      "loss: 1.018859624862671\n",
      "loss: 1.0184695720672607\n",
      "loss: 1.0180809497833252\n",
      "loss: 1.0176935195922852\n",
      "loss: 1.0173075199127197\n",
      "loss: 1.0169228315353394\n",
      "loss: 1.0165395736694336\n",
      "loss: 1.0161575078964233\n",
      "loss: 1.0157766342163086\n",
      "loss: 1.015397071838379\n",
      "loss: 1.0150187015533447\n",
      "loss: 1.0146416425704956\n",
      "loss: 1.0142656564712524\n",
      "loss: 1.0138908624649048\n",
      "loss: 1.0135172605514526\n",
      "loss: 1.0131447315216064\n",
      "loss: 1.0127733945846558\n",
      "loss: 1.0124032497406006\n",
      "loss: 1.012034296989441\n",
      "loss: 1.011666178703308\n",
      "loss: 1.0112992525100708\n",
      "loss: 1.0109333992004395\n",
      "loss: 1.0105688571929932\n",
      "loss: 1.0102051496505737\n",
      "loss: 1.0098425149917603\n",
      "loss: 1.0094808340072632\n",
      "loss: 1.0091203451156616\n",
      "loss: 1.0087610483169556\n",
      "loss: 1.008402705192566\n",
      "loss: 1.0080454349517822\n",
      "loss: 1.0076892375946045\n",
      "loss: 1.0073339939117432\n",
      "loss: 1.0069799423217773\n",
      "loss: 1.0066269636154175\n",
      "loss: 1.006274938583374\n",
      "loss: 1.005924105644226\n",
      "loss: 1.0055742263793945\n",
      "loss: 1.0052255392074585\n",
      "loss: 1.0048779249191284\n",
      "loss: 1.0045312643051147\n",
      "loss: 1.0041859149932861\n",
      "loss: 1.0038416385650635\n",
      "loss: 1.0034981966018677\n",
      "loss: 1.0031561851501465\n",
      "loss: 1.0028152465820312\n",
      "loss: 1.002475619316101\n",
      "loss: 1.0021368265151978\n",
      "loss: 1.001799464225769\n",
      "loss: 1.0014631748199463\n",
      "loss: 1.001128077507019\n",
      "loss: 1.0007942914962769\n",
      "loss: 1.0004616975784302\n",
      "loss: 1.0001304149627686\n",
      "loss: 0.9998002052307129\n",
      "loss: 0.9994714856147766\n",
      "loss: 0.9991440176963806\n",
      "loss: 0.9988178610801697\n",
      "loss: 0.9984930157661438\n",
      "loss: 0.9981695413589478\n",
      "loss: 0.9978474974632263\n",
      "loss: 0.9975267648696899\n",
      "loss: 0.9972074627876282\n",
      "loss: 0.9968897104263306\n",
      "loss: 0.9965734481811523\n",
      "loss: 0.996258556842804\n",
      "loss: 0.9959450960159302\n",
      "loss: 0.9956332445144653\n",
      "loss: 0.9953230023384094\n",
      "loss: 0.9950141906738281\n",
      "loss: 0.9947071075439453\n",
      "loss: 0.9944015741348267\n",
      "loss: 0.9940977096557617\n",
      "loss: 0.9937955141067505\n",
      "loss: 0.9934949278831482\n",
      "loss: 0.9931961297988892\n",
      "loss: 0.9928989410400391\n",
      "loss: 0.9926034808158875\n",
      "loss: 0.9923099875450134\n",
      "loss: 0.992017924785614\n",
      "loss: 0.9917280077934265\n",
      "loss: 0.9914396405220032\n",
      "loss: 0.991153359413147\n",
      "loss: 0.9908687472343445\n",
      "loss: 0.9905862212181091\n",
      "loss: 0.9903053641319275\n",
      "loss: 0.9900264739990234\n",
      "loss: 0.9897493720054626\n",
      "loss: 0.9894742965698242\n",
      "loss: 0.9892011880874634\n",
      "loss: 0.9889299273490906\n",
      "loss: 0.9886606335639954\n",
      "loss: 0.988393247127533\n",
      "loss: 0.9881278872489929\n",
      "loss: 0.9878644943237305\n",
      "loss: 0.9876031279563904\n",
      "loss: 0.9873436093330383\n",
      "loss: 0.9870861172676086\n",
      "loss: 0.9868307113647461\n",
      "loss: 0.9865772128105164\n",
      "loss: 0.9863256812095642\n",
      "loss: 0.9860760569572449\n",
      "loss: 0.9858285188674927\n",
      "loss: 0.9855828285217285\n",
      "loss: 0.9853391647338867\n",
      "loss: 0.9850975275039673\n",
      "loss: 0.9848577380180359\n",
      "loss: 0.9846199154853821\n",
      "loss: 0.9843840003013611\n",
      "loss: 0.9841499328613281\n",
      "loss: 0.9839178919792175\n",
      "loss: 0.9836878180503845\n",
      "loss: 0.9834592938423157\n",
      "loss: 0.9832329154014587\n",
      "loss: 0.9830083250999451\n",
      "loss: 0.9827856421470642\n",
      "loss: 0.9825646281242371\n",
      "loss: 0.9823455214500427\n",
      "loss: 0.9821281433105469\n",
      "loss: 0.9819125533103943\n",
      "loss: 0.9816986322402954\n",
      "loss: 0.9814864993095398\n",
      "loss: 0.9812759757041931\n",
      "loss: 0.9810672998428345\n",
      "loss: 0.98086017370224\n",
      "loss: 0.9806547164916992\n",
      "loss: 0.9804508090019226\n",
      "loss: 0.9802484512329102\n",
      "loss: 0.9800478219985962\n",
      "loss: 0.9798486828804016\n",
      "loss: 0.9796510934829712\n",
      "loss: 0.9794549942016602\n",
      "loss: 0.9792604446411133\n",
      "loss: 0.9790672659873962\n",
      "loss: 0.978875458240509\n",
      "loss: 0.9786850214004517\n",
      "loss: 0.9784960746765137\n",
      "loss: 0.9783084988594055\n",
      "loss: 0.978122353553772\n",
      "loss: 0.9779373407363892\n",
      "loss: 0.9777536392211914\n",
      "loss: 0.9775713086128235\n",
      "loss: 0.9773902893066406\n",
      "loss: 0.9772103428840637\n",
      "loss: 0.9770316481590271\n",
      "loss: 0.976854145526886\n",
      "loss: 0.9766778349876404\n",
      "loss: 0.9765025973320007\n",
      "loss: 0.9763284921646118\n",
      "loss: 0.9761554598808289\n",
      "loss: 0.9759834408760071\n",
      "loss: 0.9758127331733704\n",
      "loss: 0.9756428003311157\n",
      "loss: 0.9754740595817566\n",
      "loss: 0.9753061532974243\n",
      "loss: 0.9751392602920532\n",
      "loss: 0.9749734997749329\n",
      "loss: 0.9748083353042603\n",
      "loss: 0.9746443629264832\n",
      "loss: 0.9744812250137329\n",
      "loss: 0.9743189215660095\n",
      "loss: 0.9741576313972473\n",
      "loss: 0.9739971160888672\n",
      "loss: 0.9738373160362244\n",
      "loss: 0.9736782908439636\n",
      "loss: 0.9735202789306641\n",
      "loss: 0.973362922668457\n",
      "loss: 0.9732064604759216\n",
      "loss: 0.9730505347251892\n",
      "loss: 0.972895622253418\n",
      "loss: 0.9727411270141602\n",
      "loss: 0.9725875854492188\n",
      "loss: 0.9724346399307251\n",
      "loss: 0.972282350063324\n",
      "loss: 0.9721307754516602\n",
      "loss: 0.9719799160957336\n",
      "loss: 0.9718295931816101\n",
      "loss: 0.9716798663139343\n",
      "loss: 0.9715309143066406\n",
      "loss: 0.9713824391365051\n",
      "loss: 0.9712346196174622\n",
      "loss: 0.9710874557495117\n",
      "loss: 0.9709407687187195\n",
      "loss: 0.970794677734375\n",
      "loss: 0.9706491827964783\n",
      "loss: 0.9705042243003845\n",
      "loss: 0.970359742641449\n",
      "loss: 0.9702157974243164\n",
      "loss: 0.9700724482536316\n",
      "loss: 0.9699295163154602\n",
      "loss: 0.9697872400283813\n",
      "loss: 0.9696452021598816\n",
      "loss: 0.9695038199424744\n",
      "loss: 0.9693629145622253\n",
      "loss: 0.9692224860191345\n",
      "loss: 0.9690824747085571\n",
      "loss: 0.9689428210258484\n",
      "loss: 0.9688038229942322\n",
      "loss: 0.9686651229858398\n",
      "loss: 0.9685268402099609\n",
      "loss: 0.9683891534805298\n",
      "loss: 0.968251645565033\n",
      "loss: 0.9681146740913391\n",
      "loss: 0.9679781198501587\n",
      "loss: 0.9678419232368469\n",
      "loss: 0.9677062034606934\n",
      "loss: 0.9675709009170532\n",
      "loss: 0.9674358367919922\n",
      "loss: 0.9673011898994446\n",
      "loss: 0.9671669006347656\n",
      "loss: 0.9670330286026001\n",
      "loss: 0.9668995141983032\n",
      "loss: 0.966766357421875\n",
      "loss: 0.9666336178779602\n",
      "loss: 0.9665012359619141\n",
      "loss: 0.9663691520690918\n",
      "loss: 0.966237485408783\n",
      "loss: 0.9661060571670532\n",
      "loss: 0.9659749865531921\n",
      "loss: 0.9658441543579102\n",
      "loss: 0.9657139182090759\n",
      "loss: 0.9655837416648865\n",
      "loss: 0.9654539227485657\n",
      "loss: 0.9653244018554688\n",
      "loss: 0.9651952981948853\n",
      "loss: 0.9650665521621704\n",
      "loss: 0.9649379849433899\n",
      "loss: 0.9648098349571228\n",
      "loss: 0.9646818041801453\n",
      "loss: 0.9645542502403259\n",
      "loss: 0.9644269347190857\n",
      "loss: 0.9642999768257141\n",
      "loss: 0.9641731381416321\n",
      "loss: 0.9640466570854187\n",
      "loss: 0.9639205932617188\n",
      "loss: 0.9637946486473083\n",
      "loss: 0.9636690020561218\n",
      "loss: 0.963543713092804\n",
      "loss: 0.9634186029434204\n",
      "loss: 0.9632938504219055\n",
      "loss: 0.963169276714325\n",
      "loss: 0.9630451202392578\n",
      "loss: 0.9629209637641907\n",
      "loss: 0.9627973437309265\n",
      "loss: 0.9626738429069519\n",
      "loss: 0.9625507593154907\n",
      "loss: 0.9624277353286743\n",
      "loss: 0.9623050689697266\n",
      "loss: 0.9621826410293579\n",
      "loss: 0.9620605707168579\n",
      "loss: 0.9619385004043579\n",
      "loss: 0.9618167877197266\n",
      "loss: 0.9616953730583191\n",
      "loss: 0.9615740776062012\n",
      "loss: 0.9614532589912415\n",
      "loss: 0.9613324403762817\n",
      "loss: 0.9612120985984802\n",
      "loss: 0.9610918164253235\n",
      "loss: 0.9609718322753906\n",
      "loss: 0.9608520865440369\n",
      "loss: 0.9607324600219727\n",
      "loss: 0.9606132507324219\n",
      "loss: 0.9604941606521606\n",
      "loss: 0.9603754281997681\n",
      "loss: 0.9602567553520203\n",
      "loss: 0.9601383209228516\n",
      "loss: 0.9600201845169067\n",
      "loss: 0.9599022269248962\n",
      "loss: 0.9597845077514648\n",
      "loss: 0.9596670269966125\n",
      "loss: 0.9595497250556946\n",
      "loss: 0.9594327807426453\n",
      "loss: 0.9593158960342407\n",
      "loss: 0.9591993689537048\n",
      "loss: 0.9590829014778137\n",
      "loss: 0.9589667320251465\n",
      "loss: 0.9588506817817688\n",
      "loss: 0.9587349891662598\n",
      "loss: 0.9586195349693298\n",
      "loss: 0.9585041403770447\n",
      "loss: 0.9583889842033386\n",
      "loss: 0.9582740664482117\n",
      "loss: 0.9581594467163086\n",
      "loss: 0.9580448269844055\n",
      "loss: 0.9579305648803711\n",
      "loss: 0.9578165411949158\n",
      "loss: 0.95770263671875\n",
      "loss: 0.9575889706611633\n",
      "loss: 0.957475483417511\n",
      "loss: 0.957362174987793\n",
      "loss: 0.9572491645812988\n",
      "loss: 0.957136332988739\n",
      "loss: 0.9570236206054688\n",
      "loss: 0.9569110870361328\n",
      "loss: 0.9567988514900208\n",
      "loss: 0.956686794757843\n",
      "loss: 0.9565749168395996\n",
      "loss: 0.9564631581306458\n",
      "loss: 0.9563516974449158\n",
      "loss: 0.9562403559684753\n",
      "loss: 0.956129252910614\n",
      "loss: 0.9560183882713318\n",
      "loss: 0.9559076428413391\n",
      "loss: 0.9557971954345703\n",
      "loss: 0.9556867480278015\n",
      "loss: 0.9555765390396118\n",
      "loss: 0.9554665684700012\n",
      "loss: 0.955356776714325\n",
      "loss: 0.9552473425865173\n",
      "loss: 0.9551378488540649\n",
      "loss: 0.9550285339355469\n",
      "loss: 0.9549196362495422\n",
      "loss: 0.9548107385635376\n",
      "loss: 0.9547020196914673\n",
      "loss: 0.9545934796333313\n",
      "loss: 0.9544852375984192\n",
      "loss: 0.9543771743774414\n",
      "loss: 0.9542692303657532\n",
      "loss: 0.9541614651679993\n",
      "loss: 0.9540538191795349\n",
      "loss: 0.9539465308189392\n",
      "loss: 0.9538392424583435\n",
      "loss: 0.9537321925163269\n",
      "loss: 0.9536253213882446\n",
      "loss: 0.9535186886787415\n",
      "loss: 0.9534120559692383\n",
      "loss: 0.953305721282959\n",
      "loss: 0.953199565410614\n",
      "loss: 0.9530936479568481\n",
      "loss: 0.9529876708984375\n",
      "loss: 0.9528819918632507\n",
      "loss: 0.9527765512466431\n",
      "loss: 0.952671229839325\n",
      "loss: 0.9525661468505859\n",
      "loss: 0.9524611830711365\n",
      "loss: 0.9523563385009766\n",
      "loss: 0.9522518515586853\n",
      "loss: 0.9521473050117493\n",
      "loss: 0.9520430564880371\n",
      "loss: 0.951938807964325\n",
      "loss: 0.9518348574638367\n",
      "loss: 0.9517311453819275\n",
      "loss: 0.9516274333000183\n",
      "loss: 0.9515239596366882\n",
      "loss: 0.951420783996582\n",
      "loss: 0.9513176083564758\n",
      "loss: 0.951214611530304\n",
      "loss: 0.951111912727356\n",
      "loss: 0.951009213924408\n",
      "loss: 0.9509067535400391\n",
      "loss: 0.9508044123649597\n",
      "loss: 0.9507023096084595\n",
      "loss: 0.9506003260612488\n",
      "loss: 0.9504985213279724\n",
      "loss: 0.9503969550132751\n",
      "loss: 0.9502953886985779\n",
      "loss: 0.9501940011978149\n",
      "loss: 0.9500927925109863\n",
      "loss: 0.9499918222427368\n",
      "loss: 0.9498909115791321\n",
      "loss: 0.9497901797294617\n",
      "loss: 0.9496896862983704\n",
      "loss: 0.9495892524719238\n",
      "loss: 0.9494890570640564\n",
      "loss: 0.9493889212608337\n",
      "loss: 0.9492889642715454\n",
      "loss: 0.9491891860961914\n",
      "loss: 0.9490894675254822\n",
      "loss: 0.9489900469779968\n",
      "loss: 0.9488908052444458\n",
      "loss: 0.9487916231155396\n",
      "loss: 0.9486926198005676\n",
      "loss: 0.9485937356948853\n",
      "loss: 0.9484950304031372\n",
      "loss: 0.9483965039253235\n",
      "loss: 0.9482980966567993\n",
      "loss: 0.9481998682022095\n",
      "loss: 0.9481016993522644\n",
      "loss: 0.9480037689208984\n",
      "loss: 0.947905957698822\n",
      "loss: 0.9478082656860352\n",
      "loss: 0.9477108120918274\n",
      "loss: 0.9476134181022644\n",
      "loss: 0.947516143321991\n",
      "loss: 0.9474191665649414\n",
      "loss: 0.9473223090171814\n",
      "loss: 0.9472255706787109\n",
      "loss: 0.9471288919448853\n",
      "loss: 0.9470323920249939\n",
      "loss: 0.9469360709190369\n",
      "loss: 0.9468398094177246\n",
      "loss: 0.9467437863349915\n",
      "loss: 0.9466478228569031\n",
      "loss: 0.9465520977973938\n",
      "loss: 0.9464565515518188\n",
      "loss: 0.9463610053062439\n",
      "loss: 0.9462656378746033\n",
      "loss: 0.946170449256897\n",
      "loss: 0.946075439453125\n",
      "loss: 0.9459805488586426\n",
      "loss: 0.9458856582641602\n",
      "loss: 0.9457910656929016\n",
      "loss: 0.9456965327262878\n",
      "loss: 0.9456022381782532\n",
      "loss: 0.9455080032348633\n",
      "loss: 0.9454140067100525\n",
      "loss: 0.9453200697898865\n",
      "loss: 0.9452263116836548\n",
      "loss: 0.9451326727867126\n",
      "loss: 0.9450389742851257\n",
      "loss: 0.944945752620697\n",
      "loss: 0.9448524713516235\n",
      "loss: 0.9447593688964844\n",
      "loss: 0.9446663856506348\n",
      "loss: 0.9445735216140747\n",
      "loss: 0.9444808959960938\n",
      "loss: 0.9443883299827576\n",
      "loss: 0.9442958235740662\n",
      "loss: 0.9442035555839539\n",
      "loss: 0.9441113471984863\n",
      "loss: 0.9440193176269531\n",
      "loss: 0.9439274668693542\n",
      "loss: 0.9438356757164001\n",
      "loss: 0.9437441229820251\n",
      "loss: 0.9436525702476501\n",
      "loss: 0.9435611963272095\n",
      "loss: 0.9434699416160583\n",
      "loss: 0.9433788657188416\n",
      "loss: 0.9432878494262695\n",
      "loss: 0.9431970715522766\n",
      "loss: 0.9431062936782837\n",
      "loss: 0.9430158734321594\n",
      "loss: 0.9429252743721008\n",
      "loss: 0.9428348541259766\n",
      "loss: 0.9427447319030762\n",
      "loss: 0.9426546096801758\n",
      "loss: 0.9425647854804993\n",
      "loss: 0.9424749612808228\n",
      "loss: 0.9423853158950806\n",
      "loss: 0.9422956705093384\n",
      "loss: 0.9422062039375305\n",
      "loss: 0.942116916179657\n",
      "loss: 0.942027747631073\n",
      "loss: 0.9419386982917786\n",
      "loss: 0.9418498277664185\n",
      "loss: 0.9417610168457031\n",
      "loss: 0.9416723251342773\n",
      "loss: 0.9415837526321411\n",
      "loss: 0.9414953589439392\n",
      "loss: 0.9414070248603821\n",
      "loss: 0.941318929195404\n",
      "loss: 0.9412307739257812\n",
      "loss: 0.9411429762840271\n",
      "loss: 0.9410551190376282\n",
      "loss: 0.9409673810005188\n",
      "loss: 0.9408798217773438\n",
      "loss: 0.9407925009727478\n",
      "loss: 0.9407051205635071\n",
      "loss: 0.9406179785728455\n",
      "loss: 0.9405307769775391\n",
      "loss: 0.9404438138008118\n",
      "loss: 0.9403570294380188\n",
      "loss: 0.9402703642845154\n",
      "loss: 0.9401837587356567\n",
      "loss: 0.9400972723960876\n",
      "loss: 0.9400109648704529\n",
      "loss: 0.9399246573448181\n",
      "loss: 0.9398385882377625\n",
      "loss: 0.9397525787353516\n",
      "loss: 0.9396666884422302\n",
      "loss: 0.9395809173583984\n",
      "loss: 0.9394952654838562\n",
      "loss: 0.9394097328186035\n",
      "loss: 0.9393243789672852\n",
      "loss: 0.9392391443252563\n",
      "loss: 0.9391539692878723\n",
      "loss: 0.9390687942504883\n",
      "loss: 0.9389839172363281\n",
      "loss: 0.9388991594314575\n",
      "loss: 0.9388143420219421\n",
      "loss: 0.9387297630310059\n",
      "loss: 0.9386453628540039\n",
      "loss: 0.938560962677002\n",
      "loss: 0.9384767413139343\n",
      "loss: 0.9383925795555115\n",
      "loss: 0.9383086562156677\n",
      "loss: 0.9382246136665344\n",
      "loss: 0.938140869140625\n",
      "loss: 0.9380571246147156\n",
      "loss: 0.9379736185073853\n",
      "loss: 0.9378901720046997\n",
      "loss: 0.9378069043159485\n",
      "loss: 0.9377235770225525\n",
      "loss: 0.9376404881477356\n",
      "loss: 0.9375573992729187\n",
      "loss: 0.9374745488166809\n",
      "loss: 0.9373918771743774\n",
      "loss: 0.9373090863227844\n",
      "loss: 0.9372264742851257\n",
      "loss: 0.9371441006660461\n",
      "loss: 0.9370617866516113\n",
      "loss: 0.9369795918464661\n",
      "loss: 0.9368974566459656\n",
      "loss: 0.9368153810501099\n",
      "loss: 0.9367335438728333\n",
      "loss: 0.9366517066955566\n",
      "loss: 0.9365701079368591\n",
      "loss: 0.9364884495735168\n",
      "loss: 0.9364070296287537\n",
      "loss: 0.9363256692886353\n",
      "loss: 0.9362444281578064\n",
      "loss: 0.9361632466316223\n",
      "loss: 0.9360823035240173\n",
      "loss: 0.9360013008117676\n",
      "loss: 0.9359205365180969\n",
      "loss: 0.935839831829071\n",
      "loss: 0.9357591867446899\n",
      "loss: 0.9356786608695984\n",
      "loss: 0.9355983138084412\n",
      "loss: 0.9355179667472839\n",
      "loss: 0.935437798500061\n",
      "loss: 0.9353576898574829\n",
      "loss: 0.9352777600288391\n",
      "loss: 0.9351979494094849\n",
      "loss: 0.9351181387901306\n",
      "loss: 0.9350385069847107\n",
      "loss: 0.9349589347839355\n",
      "loss: 0.93487948179245\n",
      "loss: 0.9348001480102539\n",
      "loss: 0.9347209930419922\n",
      "loss: 0.9346418380737305\n",
      "loss: 0.9345626831054688\n",
      "loss: 0.9344837069511414\n",
      "loss: 0.9344049692153931\n",
      "loss: 0.934326171875\n",
      "loss: 0.934247612953186\n",
      "loss: 0.9341692328453064\n",
      "loss: 0.9340906143188477\n",
      "loss: 0.9340123534202576\n",
      "loss: 0.9339341521263123\n",
      "loss: 0.9338561296463013\n",
      "loss: 0.9337779879570007\n",
      "loss: 0.9337000846862793\n",
      "loss: 0.9336223602294922\n",
      "loss: 0.9335445761680603\n",
      "loss: 0.933466911315918\n",
      "loss: 0.9333894848823547\n",
      "loss: 0.9333120584487915\n",
      "loss: 0.9332348108291626\n",
      "loss: 0.9331575632095337\n",
      "loss: 0.9330803155899048\n",
      "loss: 0.9330033659934998\n",
      "loss: 0.9329264760017395\n",
      "loss: 0.9328497052192688\n",
      "loss: 0.9327729344367981\n",
      "loss: 0.9326963424682617\n",
      "loss: 0.9326196908950806\n",
      "loss: 0.9325433969497681\n",
      "loss: 0.932466983795166\n",
      "loss: 0.9323908090591431\n",
      "loss: 0.9323145747184753\n",
      "loss: 0.9322386980056763\n",
      "loss: 0.9321627020835876\n",
      "loss: 0.9320868849754333\n",
      "loss: 0.932011067867279\n",
      "loss: 0.9319354891777039\n",
      "loss: 0.9318599104881287\n",
      "loss: 0.931784451007843\n",
      "loss: 0.9317089915275574\n",
      "loss: 0.9316337704658508\n",
      "loss: 0.9315586090087891\n",
      "loss: 0.9314836859703064\n",
      "loss: 0.9314085245132446\n",
      "loss: 0.9313337206840515\n",
      "loss: 0.9312588572502136\n",
      "loss: 0.9311842322349548\n",
      "loss: 0.931109607219696\n",
      "loss: 0.931035041809082\n",
      "loss: 0.9309606552124023\n",
      "loss: 0.9308863878250122\n",
      "loss: 0.9308120608329773\n",
      "loss: 0.9307379126548767\n",
      "loss: 0.9306638836860657\n",
      "loss: 0.9305898547172546\n",
      "loss: 0.9305161833763123\n",
      "loss: 0.9304424524307251\n",
      "loss: 0.9303687214851379\n",
      "loss: 0.9302951693534851\n",
      "loss: 0.9302215576171875\n",
      "loss: 0.9301482439041138\n",
      "loss: 0.9300748705863953\n",
      "loss: 0.9300016760826111\n",
      "loss: 0.9299286007881165\n",
      "loss: 0.9298555254936218\n",
      "loss: 0.9297825694084167\n",
      "loss: 0.9297098517417908\n",
      "loss: 0.9296369552612305\n",
      "loss: 0.9295642971992493\n",
      "loss: 0.9294916987419128\n",
      "loss: 0.929419219493866\n",
      "loss: 0.9293468594551086\n",
      "loss: 0.9292745590209961\n",
      "loss: 0.9292022585868835\n",
      "loss: 0.9291301965713501\n",
      "loss: 0.9290580749511719\n",
      "loss: 0.9289861917495728\n",
      "loss: 0.9289142489433289\n",
      "loss: 0.9288425445556641\n",
      "loss: 0.9287708401679993\n",
      "loss: 0.9286991953849792\n",
      "loss: 0.9286277890205383\n",
      "loss: 0.9285562634468079\n",
      "loss: 0.9284849166870117\n",
      "loss: 0.9284136891365051\n",
      "loss: 0.9283426403999329\n",
      "loss: 0.928271472454071\n",
      "loss: 0.9282005429267883\n",
      "loss: 0.9281296133995056\n",
      "loss: 0.9280588030815125\n",
      "loss: 0.9279880523681641\n",
      "loss: 0.92791748046875\n",
      "loss: 0.9278468489646912\n",
      "loss: 0.9277763366699219\n",
      "loss: 0.9277059435844421\n",
      "loss: 0.9276357889175415\n",
      "loss: 0.9275655150413513\n",
      "loss: 0.9274954199790955\n",
      "loss: 0.9274253249168396\n",
      "loss: 0.9273554086685181\n",
      "loss: 0.9272854924201965\n",
      "loss: 0.9272157549858093\n",
      "loss: 0.9271459579467773\n",
      "loss: 0.927076518535614\n",
      "loss: 0.9270069003105164\n",
      "loss: 0.9269375205039978\n",
      "loss: 0.9268680810928345\n",
      "loss: 0.9267988204956055\n",
      "loss: 0.9267296195030212\n",
      "loss: 0.9266605377197266\n",
      "loss: 0.9265915155410767\n",
      "loss: 0.926522433757782\n",
      "loss: 0.926453709602356\n",
      "loss: 0.9263849258422852\n",
      "loss: 0.9263162612915039\n",
      "loss: 0.9262475967407227\n",
      "loss: 0.9261791110038757\n",
      "loss: 0.9261106848716736\n",
      "loss: 0.9260422587394714\n",
      "loss: 0.9259740114212036\n",
      "loss: 0.9259058237075806\n",
      "loss: 0.9258376955986023\n",
      "loss: 0.9257696270942688\n",
      "loss: 0.9257017374038696\n",
      "loss: 0.9256338477134705\n",
      "loss: 0.9255660176277161\n",
      "loss: 0.9254983067512512\n",
      "loss: 0.9254305958747864\n",
      "loss: 0.9253631830215454\n",
      "loss: 0.9252956509590149\n",
      "loss: 0.9252282977104187\n",
      "loss: 0.9251608848571777\n",
      "loss: 0.9250936508178711\n",
      "loss: 0.925026535987854\n",
      "loss: 0.9249594807624817\n",
      "loss: 0.9248925447463989\n",
      "loss: 0.9248256683349609\n",
      "loss: 0.9247588515281677\n",
      "loss: 0.9246920943260193\n",
      "loss: 0.9246253967285156\n",
      "loss: 0.9245587587356567\n",
      "loss: 0.924492359161377\n",
      "loss: 0.9244258999824524\n",
      "loss: 0.9243595004081726\n",
      "loss: 0.9242931604385376\n",
      "loss: 0.9242270588874817\n",
      "loss: 0.9241609573364258\n",
      "loss: 0.9240949749946594\n",
      "loss: 0.9240289926528931\n",
      "loss: 0.9239630103111267\n",
      "loss: 0.9238972663879395\n",
      "loss: 0.9238314628601074\n",
      "loss: 0.9237658381462097\n",
      "loss: 0.9237003326416016\n",
      "loss: 0.9236348271369934\n",
      "loss: 0.92356938123703\n",
      "loss: 0.9235040545463562\n",
      "loss: 0.9234387278556824\n",
      "loss: 0.9233736395835876\n",
      "loss: 0.9233084917068481\n",
      "loss: 0.9232434630393982\n",
      "loss: 0.923178493976593\n",
      "loss: 0.9231136441230774\n",
      "loss: 0.9230487942695618\n",
      "loss: 0.9229839444160461\n",
      "loss: 0.9229193925857544\n",
      "loss: 0.9228549003601074\n",
      "loss: 0.9227903485298157\n",
      "loss: 0.9227258563041687\n",
      "loss: 0.9226616024971008\n",
      "loss: 0.922597348690033\n",
      "loss: 0.9225330352783203\n",
      "loss: 0.9224689602851868\n",
      "loss: 0.9224048852920532\n",
      "loss: 0.922340989112854\n",
      "loss: 0.9222770929336548\n",
      "loss: 0.9222131967544556\n",
      "loss: 0.9221494793891907\n",
      "loss: 0.9220857620239258\n",
      "loss: 0.9220220446586609\n",
      "loss: 0.9219586253166199\n",
      "loss: 0.9218952059745789\n",
      "loss: 0.9218317866325378\n",
      "loss: 0.9217684864997864\n",
      "loss: 0.9217052459716797\n",
      "loss: 0.9216421246528625\n",
      "loss: 0.9215790033340454\n",
      "loss: 0.921515941619873\n",
      "loss: 0.9214529991149902\n",
      "loss: 0.921390175819397\n",
      "loss: 0.9213274121284485\n",
      "loss: 0.9212646484375\n",
      "loss: 0.9212020039558411\n",
      "loss: 0.9211394190788269\n",
      "loss: 0.9210768938064575\n",
      "loss: 0.9210144281387329\n",
      "loss: 0.9209520220756531\n",
      "loss: 0.9208897948265076\n",
      "loss: 0.9208275675773621\n",
      "loss: 0.9207653999328613\n",
      "loss: 0.9207033514976501\n",
      "loss: 0.9206413626670837\n",
      "loss: 0.9205794334411621\n",
      "loss: 0.9205175638198853\n",
      "loss: 0.9204557538032532\n",
      "loss: 0.9203939437866211\n",
      "loss: 0.9203323721885681\n",
      "loss: 0.9202707409858704\n",
      "loss: 0.9202093482017517\n",
      "loss: 0.9201478958129883\n",
      "loss: 0.9200865030288696\n",
      "loss: 0.9200251698493958\n",
      "loss: 0.9199639558792114\n",
      "loss: 0.9199028015136719\n",
      "loss: 0.9198417663574219\n",
      "loss: 0.9197807312011719\n",
      "loss: 0.9197196960449219\n",
      "loss: 0.9196588397026062\n",
      "loss: 0.9195981025695801\n",
      "loss: 0.9195372462272644\n",
      "loss: 0.9194766879081726\n",
      "loss: 0.919416069984436\n",
      "loss: 0.9193555116653442\n",
      "loss: 0.9192950129508972\n",
      "loss: 0.9192346930503845\n",
      "loss: 0.919174313545227\n",
      "loss: 0.9191141128540039\n",
      "loss: 0.919053852558136\n",
      "loss: 0.9189937710762024\n",
      "loss: 0.9189336895942688\n",
      "loss: 0.9188737273216248\n",
      "loss: 0.9188138246536255\n",
      "loss: 0.9187540411949158\n",
      "loss: 0.9186941385269165\n",
      "loss: 0.9186344146728516\n",
      "loss: 0.9185747504234314\n",
      "loss: 0.9185152053833008\n",
      "loss: 0.9184557199478149\n",
      "loss: 0.9183962941169739\n",
      "loss: 0.9183368682861328\n",
      "loss: 0.9182775616645813\n",
      "loss: 0.9182183146476746\n",
      "loss: 0.9181591272354126\n",
      "loss: 0.9181000590324402\n",
      "loss: 0.9180410504341125\n",
      "loss: 0.9179820418357849\n",
      "loss: 0.9179231524467468\n",
      "loss: 0.9178642630577087\n",
      "loss: 0.9178054928779602\n",
      "loss: 0.9177468419075012\n",
      "loss: 0.9176881909370422\n",
      "loss: 0.9176296591758728\n",
      "loss: 0.9175710678100586\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    y_pred = model(x)[\"personBbox\"]\n",
    "    loss = T.nn.functional.mse_loss(y_pred, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# import torch as T\n",
    "# import torch.nn as nn\n",
    "# import pytorch_lightning as pl\n",
    "# from motion_capture.model.models import VQVAE, VQVAEHead\n",
    "\n",
    "# class VisionModel(pl.LightningModule):\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "        \n",
    "#         backbone: str,\n",
    "#         vqvae: dict,\n",
    "#         heads: dict,\n",
    "        \n",
    "#         # - training parameters\n",
    "#         optimizer: T.optim.Optimizer = None,\n",
    "#         optimizer_kwargs: dict = None,\n",
    "#         lr_scheduler_warmup_epochs: int = None,\n",
    "#         lr_scheduler: T.optim.lr_scheduler = None,\n",
    "#         lr_scheduler_kwargs: dict = None\n",
    "        \n",
    "#         # - loss scale parameters\n",
    "#         reconstruction_loss_scale: float = 1.0,\n",
    "#         codebook_loss_scale: float = 1.0,\n",
    "#         prediction_loss_scales: dict = None\n",
    "#         ):\n",
    "        \n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "        \n",
    "#         self.backbone = timm.create_model(backbone, pretrained=False, features_only=True)\n",
    "#         self.vqvae = vqvae = VQVAE(**vqvae)\n",
    "        \n",
    "#         self.heads = nn.ModuleDict({k: VQVAEHead(**v) for k, v in heads.items()})\n",
    "        \n",
    "#     def forward(self, x, skip_backbone=False):\n",
    "        \n",
    "#         if not skip_backbone:\n",
    "#             backbone_out = self.backbone(x)[-1].flatten(2).permute(0, 2, 1)\n",
    "#         else:\n",
    "#             backbone_out = x\n",
    "        \n",
    "#         vqvae_out = self.vqvae(backbone_out)\n",
    "        \n",
    "#         heads_out = {}\n",
    "#         for k in self.heads:\n",
    "#             heads_out[k] = self.heads[k](vqvae_out[\"codebook_onehots\"])\n",
    "        \n",
    "#         return {\n",
    "#             \"heads\": heads_out, \n",
    "#             \"vqvae\": vqvae_out\n",
    "#         }\n",
    "    \n",
    "#     def get_losses(self, \n",
    "#         heads_out, heads_targets,\n",
    "#         vqvae_out, vqvae_target):\n",
    "        \n",
    "#         reconstruction_loss, codebook_loss = self.vqvae.compute_loss(vqvae_reconstruction_target, vqvae_out[\"reconstruction\"], vqvae_out[\"z\"], vqvae_out[\"codebook_indecies\"])\n",
    "#         prediction_losses = {\n",
    "#             k: self.hparams.prediction_loss_scales.get(k, 1) * self.heads[k].compute_loss(heads_targets[k], heads_out[k]) \n",
    "#             for k in self.heads\n",
    "#         }\n",
    "        \n",
    "#         return {\n",
    "#             \"vqvae-reconstruction\": reconstruction_loss * self.hparams.reconstruction_loss_scale,\n",
    "#             \"vqvae-codebook\": codebook_loss * self.hparams.codebook_loss_scale,\n",
    "#             **prediction_losses\n",
    "#         }\n",
    "    \n",
    "#     # def head_wise_step(self, batch, mode):\n",
    "        \n",
    "#     #     x, y = batch\n",
    "#     #     outputs = self(x)\n",
    "#     #     losses = {}\n",
    "#     #     for k, head_output in outputs.items():\n",
    "#     #         loss = self.compute_loss(head_output, y[k], self.hparams.heads[k][\"loss_fn\"])\n",
    "#     #         losses[k] = loss\n",
    "#     #         self.log(f\"{mode}_loss_{k}\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "#     #     self.log(f\"{mode}_loss\", sum(losses.values()) / len(losses), on_step=False, on_epoch=True, prog_bar=True)\n",
    "#     #     return losses\n",
    "    \n",
    "#     # def training_step(self, batch, batch_id):\n",
    "#     #     return self.head_wise_step(batch, \"train\")\n",
    "#     # def validation_step(self, batch, batch_idx):\n",
    "#     #     self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"], on_step=False, on_epoch=True, prog_bar=False)\n",
    "#     #     return self.head_wise_step(batch, \"val\")\n",
    "#     # def test_step(self, batch, batch_idx):\n",
    "#     #     return self.head_wise_step(batch, \"test\")\n",
    "    \n",
    "#     # def configure_optimizers(self):\n",
    "        \n",
    "#     #     assert self.hparams.optimizer, \"optimizer not set for training\"\n",
    "        \n",
    "#     #     opt = self.hparams.optimizer(self.parameters(), **self.hparams.optimizer_kwargs)\n",
    "        \n",
    "#     #     warmup_scheduler = T.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=self.hparams.lr_scheduler_warmup_epochs)\n",
    "#     #     scheduler = self.hparams.lr_scheduler(opt, **self.hparams.lr_scheduler_kwargs)\n",
    "#     #     lr_scheduler = T.optim.lr_scheduler.SequentialLR(opt, schedulers=[\n",
    "#     #         warmup_scheduler, \n",
    "#     #         scheduler\n",
    "#     #     ], milestones=[self.hparams.lr_scheduler_warmup_epochs])\n",
    "        \n",
    "#     #     return {\n",
    "#     #         \"optimizer\": opt,\n",
    "#     #         \"lr_scheduler\": lr_scheduler\n",
    "#     #     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967caa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VisionModel(\n",
    "#     backbone = \"timm/convnextv2_atto.fcmae_ft_in1k\",\n",
    "#     vqvae = {\n",
    "#         \"input_dim\": 320,\n",
    "#         \"codebook_dim\": 512,\n",
    "#         \"num_codebook_entries\": 1024,\n",
    "#         \"output_dim\": 320,\n",
    "#         \"codebook_sequence_length\": 15,\n",
    "#         \"output_sequence_length\": 49,\n",
    "#         \"transformer\": {\n",
    "#             \"nhead\": 2,\n",
    "#             \"num_encoder_layers\": 1,\n",
    "#             \"num_decoder_layers\": 1,\n",
    "#             \"dim_feedforward\": 512,\n",
    "#             \"dropout\": 0.1,\n",
    "#             \"activation\": \"gelu\"\n",
    "#         }\n",
    "#     },\n",
    "#     heads = {\n",
    "#         \"personBbox\": {\n",
    "#             \"input_dim\": 1024,\n",
    "#             \"output_dim\": 4,\n",
    "#             \"latent_dim\": 512,\n",
    "#             \"output_sequence_length\": 10,\n",
    "#             \"output_type\": \"continuous\",\n",
    "#             \"transformer\": {\n",
    "#                 \"nhead\": 2,\n",
    "#                 \"num_encoder_layers\": 1,\n",
    "#                 \"num_decoder_layers\": 1,\n",
    "#                 \"dim_feedforward\": 512,\n",
    "#                 \"activation\": \"gelu\"\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f3b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from motion_capture.core import benchmark\n",
    "\n",
    "# benchmark.model_speedtest(model, (3, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ced1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_net_dataset = []\n",
    "teacher_net_dataset = []\n",
    "\n",
    "from motion_capture.data.datasets import COCO2017GlobalPersonInstanceSegmentation\n",
    "person_instance_dataset = COCO2017GlobalPersonInstanceSegmentation(\n",
    "    image_folder_path = \"//192.168.2.206/data/datasets/COCO2017/images\",\n",
    "    annotation_folder_path = \"//192.168.2.206/data/datasets/COCO2017/annotations\",\n",
    "    image_shape_WH=(224, 224),\n",
    "    max_num_persons=10,\n",
    "    max_segmentation_points=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b91422-af1d-4cc8-8d69-3f90e9bd4531",
   "metadata": {},
   "source": [
    "## PyTorch Pretrained Models Speedtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as torchModels\n",
    "from motion_capture.model import models as mocapModels\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_models = {\n",
    "    \"fcos_resnet50_fpn\": torchModels.detection.fcos_resnet50_fpn(weights=torchModels.detection.FCOS_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_320_fpn\": torchModels.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=torchModels.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_fpn\": torchModels.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=torchModels.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn_v2\": torchModels.detection.fasterrcnn_resnet50_fpn_v2(weights=torchModels.detection.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn\": torchModels.detection.fasterrcnn_resnet50_fpn(weights=torchModels.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn_v2\": torchModels.detection.retinanet_resnet50_fpn_v2(weights=torchModels.detection.RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn\": torchModels.detection.retinanet_resnet50_fpn(weights=torchModels.detection.RetinaNet_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"ssd300_vgg16\": torchModels.detection.ssd300_vgg16(weights=torchModels.detection.SSD300_VGG16_Weights.COCO_V1),\n",
    "    \"ssdlite320_mobilenet_v3_large\": torchModels.detection.ssdlite320_mobilenet_v3_large(weights=torchModels.detection.SSDLite320_MobileNet_V3_Large_Weights.COCO_V1),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
