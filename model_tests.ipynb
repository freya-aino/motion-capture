{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa25ff3-8adf-4c7a-a9b0-c66b25a7c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58303382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with neck: <class 'motion_capture.model.convolution.necks.UpsampleCrossAttentionrNeck'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2244, -0.6158,  0.2961],\n",
       "         [ 0.4649, -0.6930,  0.2423],\n",
       "         [ 0.4447, -0.6111,  0.3488],\n",
       "         [ 0.4098, -0.7241,  0.2867],\n",
       "         [ 0.5571, -0.4692,  0.3012],\n",
       "         [ 0.4822, -0.7920,  0.2826],\n",
       "         [ 0.5228, -0.5817,  0.4432],\n",
       "         [ 0.4856, -0.7587,  0.4615],\n",
       "         [ 0.6528, -0.4922,  0.3782],\n",
       "         [ 0.4061, -0.6727,  0.1855]],\n",
       "\n",
       "        [[-0.4781, -0.7350, -0.2535],\n",
       "         [-0.1808, -0.8107, -0.2508],\n",
       "         [-0.1982, -0.6430, -0.1063],\n",
       "         [-0.2696, -0.7685, -0.2514],\n",
       "         [-0.1437, -0.5232, -0.2280],\n",
       "         [-0.1951, -0.8496, -0.2316],\n",
       "         [-0.1323, -0.6956, -0.0557],\n",
       "         [-0.2099, -0.7994, -0.0378],\n",
       "         [-0.0170, -0.6205, -0.1340],\n",
       "         [-0.3126, -0.7231, -0.3310]],\n",
       "\n",
       "        [[ 0.0818,  0.1004, -0.0076],\n",
       "         [ 0.3618,  0.0725,  0.0425],\n",
       "         [ 0.3894,  0.0723,  0.1568],\n",
       "         [ 0.3320,  0.0793,  0.0485],\n",
       "         [ 0.4595,  0.3672,  0.0703],\n",
       "         [ 0.3839,  0.0294,  0.0574],\n",
       "         [ 0.4556,  0.1427,  0.1961],\n",
       "         [ 0.3795,  0.0555,  0.2382],\n",
       "         [ 0.5484,  0.2429,  0.1361],\n",
       "         [ 0.3093,  0.0905, -0.0464]],\n",
       "\n",
       "        [[-0.1487, -0.5632, -0.3640],\n",
       "         [ 0.0887, -0.5406, -0.3201],\n",
       "         [ 0.1209, -0.4040, -0.2380],\n",
       "         [ 0.1024, -0.5848, -0.3691],\n",
       "         [ 0.2766, -0.3179, -0.3361],\n",
       "         [ 0.1603, -0.6882, -0.3527],\n",
       "         [ 0.1590, -0.4427, -0.1990],\n",
       "         [ 0.1365, -0.6419, -0.1636],\n",
       "         [ 0.3045, -0.4249, -0.2580],\n",
       "         [ 0.0363, -0.5363, -0.4417]],\n",
       "\n",
       "        [[-0.2072, -0.3028,  0.1456],\n",
       "         [ 0.0914, -0.3178,  0.2230],\n",
       "         [ 0.0862, -0.1214,  0.3654],\n",
       "         [ 0.0312, -0.2705,  0.2278],\n",
       "         [ 0.2833, -0.0993,  0.2499],\n",
       "         [ 0.1095, -0.3718,  0.2472],\n",
       "         [ 0.1507, -0.1921,  0.3940],\n",
       "         [ 0.1049, -0.4062,  0.4044],\n",
       "         [ 0.2538, -0.0911,  0.3300],\n",
       "         [ 0.0530, -0.3541,  0.1196]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import motion_capture\n",
    "\n",
    "from motion_capture.model import primaryModels\n",
    "\n",
    "\n",
    "model = primaryModels.FullConvModel(128, 3, 10)\n",
    "\n",
    "model(T.rand(5, 3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98450b30-8b79-4cc5-ba6d-ad5bfde103b9",
   "metadata": {},
   "source": [
    "## Interesting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b4d58cd-505d-49bc-ba36-909dc91d0906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchvision.models.detection' from 'c:\\\\Users\\\\noone\\\\Documents\\\\programming\\\\homemade-motion-capture\\\\.venv\\\\lib\\\\site-packages\\\\torchvision\\\\models\\\\detection\\\\__init__.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_interesting_models = {\n",
    "    \"vit_b_32\": models.get_model(\"vit_b_32\").to(\"cuda\"),\n",
    "}\n",
    "\n",
    "models.quantization\n",
    "models.detection\n",
    "models.segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b91422-af1d-4cc8-8d69-3f90e9bd4531",
   "metadata": {},
   "source": [
    "## PyTorch Pretrained Models Speedtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae430eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_models = {\n",
    "    \"fcn_resnet50\": models.segmentation.fcn_resnet50(weights=models.segmentation.FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"fcn_resnet101\": models.segmentation.fcn_resnet101(weights=models.segmentation.FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_resnet50\": models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_resnet101\": models.segmentation.deeplabv3_resnet101(weights=models.segmentation.DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_mobilenet_v3_large\": models.segmentation.deeplabv3_mobilenet_v3_large(weights=models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"lraspp_mobilenet_v3_large\": models.segmentation.lraspp_mobilenet_v3_large(weights=models.segmentation.LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "}\n",
    "\n",
    "object_detection_models = {\n",
    "    \"fcos_resnet50_fpn\": models.detection.fcos_resnet50_fpn(weights=models.detection.FCOS_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_320_fpn\": models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_fpn\": models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn_v2\": models.detection.fasterrcnn_resnet50_fpn_v2(weights=models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn\": models.detection.fasterrcnn_resnet50_fpn(weights=models.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn_v2\": models.detection.retinanet_resnet50_fpn_v2(weights=models.detection.RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn\": models.detection.retinanet_resnet50_fpn(weights=models.detection.RetinaNet_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"ssd300_vgg16\": models.detection.ssd300_vgg16(weights=models.detection.SSD300_VGG16_Weights.COCO_V1),\n",
    "    \"ssdlite320_mobilenet_v3_large\": models.detection.ssdlite320_mobilenet_v3_large(weights=models.detection.SSDLite320_MobileNet_V3_Large_Weights.COCO_V1),\n",
    "}\n",
    "\n",
    "instance_segmentation_models = {\n",
    "    \"maskrcnn_resnet50_fpn\": models.detection.maskrcnn_resnet50_fpn(weights=models.detection.MaskRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"maskrcnn_resnet50_fpn_v2\": models.detection.maskrcnn_resnet50_fpn_v2(weights=models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "}\n",
    "\n",
    "keypoint_detection_models = {\n",
    "    \"keypointrcnn_resnet50_fpn\": models.detection.keypointrcnn_resnet50_fpn(weights=models.detection.KeypointRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15922c-cfdb-4296-be6d-8fee39057e1e",
   "metadata": {},
   "source": [
    "## Facer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00cc738-7583-46fa-b879-7459df116164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import facer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0c0ab11-bb8d-4864-bdab-64ad4f33a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "facer_models = {\n",
    "    # \"retinaface/mobilenet\": facer.face_detector(\"retinaface/mobilenet\", device=\"cuda\"),\n",
    "    # \"retinaface/resnet\": facer.face_detector(\"retinaface/resnet50\", device=\"cuda\"),\n",
    "    \"farl/lapa/448\": facer.face_parser(\"farl/lapa/448\", device=\"cuda\"),\n",
    "    \"farl/celebm/448\": facer.face_parser(\"farl/celebm/448\", device=\"cuda\"),\n",
    "    \"farl/ibug300w/448\": facer.face_aligner(\"farl/ibug300w/448\", device=\"cuda\"),\n",
    "    \"farl/aflw19/448\": facer.face_aligner(\"farl/aflw19/448\", device=\"cuda\"),\n",
    "    \"farl/wflw/448\": facer.face_aligner(\"farl/wflw/448\", device=\"cuda\"),\n",
    "    \"farl/celeba/224\": facer.face_attr(\"farl/celeba/224\", device=\"cuda\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd38233-a520-45e1-841c-c2fab5d2561d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
