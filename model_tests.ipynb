{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# import torch as T\n",
    "# import os\n",
    "\n",
    "# pth = \"./dataset_embeddings/convnextv2_atto.fcmae_ft_in1k\"\n",
    "# num_encoding_blocks = len(os.listdir(pth))\n",
    "\n",
    "# embeddings = T.zeros(num_encoding_blocks * 250, 320, 7, 7)\n",
    "# valid = T.zeros(num_encoding_blocks * 250, dtype=T.bool)\n",
    "\n",
    "# print(f\"number of encodings: {len(os.listdir(pth))}\")\n",
    "# for (i, f) in tqdm.tqdm(enumerate(os.listdir(pth)), total=num_encoding_blocks, desc=\"Loading encodings\"):\n",
    "#     ten = T.load(os.path.join(pth, f), map_location=\"cpu\")\n",
    "#     block_size = ten.size(0)\n",
    "#     embeddings[(i * 250):(i * 250 + block_size), ...] = ten[:]\n",
    "#     valid[(i * 250):(i * 250 + block_size)] = True\n",
    "\n",
    "# T.save(embeddings, \"./dataset_embeddings/convnextv2_atto.fcmae_ft_in1k-embeddings.pt\")\n",
    "# T.save(valid,      \"./dataset_embeddings/convnextv2_atto.fcmae_ft_in1k-valid.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# import torch as T\n",
    "# import torch.nn as nn\n",
    "# import pytorch_lightning as pl\n",
    "# from motion_capture.model.models import VQVAE, VQVAEHead\n",
    "\n",
    "# class VisionModel(pl.LightningModule):\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "        \n",
    "#         backbone: str,\n",
    "#         vqvae: dict,\n",
    "#         heads: dict,\n",
    "        \n",
    "#         # - training parameters\n",
    "#         optimizer: T.optim.Optimizer = None,\n",
    "#         optimizer_kwargs: dict = None,\n",
    "#         lr_scheduler_warmup_epochs: int = None,\n",
    "#         lr_scheduler: T.optim.lr_scheduler = None,\n",
    "#         lr_scheduler_kwargs: dict = None\n",
    "        \n",
    "#         # - loss scale parameters\n",
    "#         reconstruction_loss_scale: float = 1.0,\n",
    "#         codebook_loss_scale: float = 1.0,\n",
    "#         prediction_loss_scales: dict = None\n",
    "#         ):\n",
    "        \n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "        \n",
    "#         self.backbone = timm.create_model(backbone, pretrained=False, features_only=True)\n",
    "#         self.vqvae = vqvae = VQVAE(**vqvae)\n",
    "        \n",
    "#         self.heads = nn.ModuleDict({k: VQVAEHead(**v) for k, v in heads.items()})\n",
    "        \n",
    "#     def forward(self, x, skip_backbone=False):\n",
    "        \n",
    "#         if not skip_backbone:\n",
    "#             backbone_out = self.backbone(x)[-1].flatten(2).permute(0, 2, 1)\n",
    "#         else:\n",
    "#             backbone_out = x\n",
    "        \n",
    "#         vqvae_out = self.vqvae(backbone_out)\n",
    "        \n",
    "#         heads_out = {}\n",
    "#         for k in self.heads:\n",
    "#             heads_out[k] = self.heads[k](vqvae_out[\"codebook_onehots\"])\n",
    "        \n",
    "#         return {\n",
    "#             \"heads\": heads_out, \n",
    "#             \"vqvae\": vqvae_out\n",
    "#         }\n",
    "    \n",
    "#     def get_losses(self, \n",
    "#         heads_out, heads_targets,\n",
    "#         vqvae_out, vqvae_target):\n",
    "        \n",
    "#         reconstruction_loss, codebook_loss = self.vqvae.compute_loss(vqvae_reconstruction_target, vqvae_out[\"reconstruction\"], vqvae_out[\"z\"], vqvae_out[\"codebook_indecies\"])\n",
    "#         prediction_losses = {\n",
    "#             k: self.hparams.prediction_loss_scales.get(k, 1) * self.heads[k].compute_loss(heads_targets[k], heads_out[k]) \n",
    "#             for k in self.heads\n",
    "#         }\n",
    "        \n",
    "#         return {\n",
    "#             \"vqvae-reconstruction\": reconstruction_loss * self.hparams.reconstruction_loss_scale,\n",
    "#             \"vqvae-codebook\": codebook_loss * self.hparams.codebook_loss_scale,\n",
    "#             **prediction_losses\n",
    "#         }\n",
    "    \n",
    "#     # def head_wise_step(self, batch, mode):\n",
    "        \n",
    "#     #     x, y = batch\n",
    "#     #     outputs = self(x)\n",
    "#     #     losses = {}\n",
    "#     #     for k, head_output in outputs.items():\n",
    "#     #         loss = self.compute_loss(head_output, y[k], self.hparams.heads[k][\"loss_fn\"])\n",
    "#     #         losses[k] = loss\n",
    "#     #         self.log(f\"{mode}_loss_{k}\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "#     #     self.log(f\"{mode}_loss\", sum(losses.values()) / len(losses), on_step=False, on_epoch=True, prog_bar=True)\n",
    "#     #     return losses\n",
    "    \n",
    "#     # def training_step(self, batch, batch_id):\n",
    "#     #     return self.head_wise_step(batch, \"train\")\n",
    "#     # def validation_step(self, batch, batch_idx):\n",
    "#     #     self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"], on_step=False, on_epoch=True, prog_bar=False)\n",
    "#     #     return self.head_wise_step(batch, \"val\")\n",
    "#     # def test_step(self, batch, batch_idx):\n",
    "#     #     return self.head_wise_step(batch, \"test\")\n",
    "    \n",
    "#     # def configure_optimizers(self):\n",
    "        \n",
    "#     #     assert self.hparams.optimizer, \"optimizer not set for training\"\n",
    "        \n",
    "#     #     opt = self.hparams.optimizer(self.parameters(), **self.hparams.optimizer_kwargs)\n",
    "        \n",
    "#     #     warmup_scheduler = T.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=self.hparams.lr_scheduler_warmup_epochs)\n",
    "#     #     scheduler = self.hparams.lr_scheduler(opt, **self.hparams.lr_scheduler_kwargs)\n",
    "#     #     lr_scheduler = T.optim.lr_scheduler.SequentialLR(opt, schedulers=[\n",
    "#     #         warmup_scheduler, \n",
    "#     #         scheduler\n",
    "#     #     ], milestones=[self.hparams.lr_scheduler_warmup_epochs])\n",
    "        \n",
    "#     #     return {\n",
    "#     #         \"optimizer\": opt,\n",
    "#     #         \"lr_scheduler\": lr_scheduler\n",
    "#     #     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967caa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VisionModel(\n",
    "#     backbone = \"timm/convnextv2_atto.fcmae_ft_in1k\",\n",
    "#     vqvae = {\n",
    "#         \"input_dim\": 320,\n",
    "#         \"codebook_dim\": 512,\n",
    "#         \"num_codebook_entries\": 1024,\n",
    "#         \"output_dim\": 320,\n",
    "#         \"codebook_sequence_length\": 15,\n",
    "#         \"output_sequence_length\": 49,\n",
    "#         \"transformer\": {\n",
    "#             \"nhead\": 2,\n",
    "#             \"num_encoder_layers\": 1,\n",
    "#             \"num_decoder_layers\": 1,\n",
    "#             \"dim_feedforward\": 512,\n",
    "#             \"dropout\": 0.1,\n",
    "#             \"activation\": \"gelu\"\n",
    "#         }\n",
    "#     },\n",
    "#     heads = {\n",
    "#         \"personBbox\": {\n",
    "#             \"input_dim\": 1024,\n",
    "#             \"output_dim\": 4,\n",
    "#             \"latent_dim\": 512,\n",
    "#             \"output_sequence_length\": 10,\n",
    "#             \"output_type\": \"continuous\",\n",
    "#             \"transformer\": {\n",
    "#                 \"nhead\": 2,\n",
    "#                 \"num_encoder_layers\": 1,\n",
    "#                 \"num_decoder_layers\": 1,\n",
    "#                 \"dim_feedforward\": 512,\n",
    "#                 \"activation\": \"gelu\"\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f3b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from motion_capture.core import benchmark\n",
    "\n",
    "# benchmark.model_speedtest(model, (3, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ced1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_net_dataset = []\n",
    "teacher_net_dataset = []\n",
    "\n",
    "from motion_capture.data.datasets import COCO2017GlobalPersonInstanceSegmentation\n",
    "person_instance_dataset = COCO2017GlobalPersonInstanceSegmentation(\n",
    "    image_folder_path = \"//192.168.2.206/data/datasets/COCO2017/images\",\n",
    "    annotation_folder_path = \"//192.168.2.206/data/datasets/COCO2017/annotations\",\n",
    "    image_shape_WH=(224, 224),\n",
    "    max_num_persons=10,\n",
    "    max_segmentation_points=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b91422-af1d-4cc8-8d69-3f90e9bd4531",
   "metadata": {},
   "source": [
    "## PyTorch Pretrained Models Speedtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as torchModels\n",
    "from motion_capture.model import models as mocapModels\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_models = {\n",
    "    \"fcos_resnet50_fpn\": torchModels.detection.fcos_resnet50_fpn(weights=torchModels.detection.FCOS_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_320_fpn\": torchModels.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=torchModels.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_fpn\": torchModels.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=torchModels.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn_v2\": torchModels.detection.fasterrcnn_resnet50_fpn_v2(weights=torchModels.detection.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn\": torchModels.detection.fasterrcnn_resnet50_fpn(weights=torchModels.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn_v2\": torchModels.detection.retinanet_resnet50_fpn_v2(weights=torchModels.detection.RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn\": torchModels.detection.retinanet_resnet50_fpn(weights=torchModels.detection.RetinaNet_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"ssd300_vgg16\": torchModels.detection.ssd300_vgg16(weights=torchModels.detection.SSD300_VGG16_Weights.COCO_V1),\n",
    "    \"ssdlite320_mobilenet_v3_large\": torchModels.detection.ssdlite320_mobilenet_v3_large(weights=torchModels.detection.SSDLite320_MobileNet_V3_Large_Weights.COCO_V1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc8ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 200])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_models = [\n",
    "    # \"deeplabv3_mobilenet_v3_large\",\n",
    "    # \"deeplabv3_resnet101\",\n",
    "    # \"deeplabv3_resnet50\",\n",
    "    \"fasterrcnn_mobilenet_v3_large_320_fpn\",\n",
    "    \"fasterrcnn_mobilenet_v3_large_fpn\",\n",
    "    \"fasterrcnn_resnet50_fpn\",\n",
    "    \"fasterrcnn_resnet50_fpn_v2\",\n",
    "    # \"fcn_resnet101\",\n",
    "    # \"fcn_resnet50\",\n",
    "    \"fcos_resnet50_fpn\",\n",
    "    \"keypointrcnn_resnet50_fpn\",\n",
    "    # \"lraspp_mobilenet_v3_large\",\n",
    "    \"maskrcnn_resnet50_fpn\",\n",
    "    \"maskrcnn_resnet50_fpn_v2\",\n",
    "    \"retinanet_resnet50_fpn\",\n",
    "    \"retinanet_resnet50_fpn_v2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eeb9c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backbone_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmotion_capture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m benchmark\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbackbone_models\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmarking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m torchModels\u001b[38;5;241m.\u001b[39mget_model(model)\u001b[38;5;241m.\u001b[39mbackbone\n",
      "\u001b[1;31mNameError\u001b[0m: name 'backbone_models' is not defined"
     ]
    }
   ],
   "source": [
    "from motion_capture.core import benchmark\n",
    "\n",
    "for model in backbone_models:\n",
    "    print(f\"benchmarking {model}\")\n",
    "    model = torchModels.get_model(model).backbone\n",
    "    benchmark.model_speedtest(model, (1, 3, 224, 224), ntests=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d071880",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backbone_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbackbone_models\u001b[49m:\n\u001b[0;32m      2\u001b[0m     backbone_out \u001b[38;5;241m=\u001b[39m torchModels\u001b[38;5;241m.\u001b[39mget_model(model)\u001b[38;5;241m.\u001b[39mbackbone(T\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m448\u001b[39m, \u001b[38;5;241m448\u001b[39m))\n\u001b[0;32m      3\u001b[0m     last_3_shapes \u001b[38;5;241m=\u001b[39m [backbone_out[a]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(backbone_out)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'backbone_models' is not defined"
     ]
    }
   ],
   "source": [
    "for model in backbone_models:\n",
    "    backbone_out = torchModels.get_model(model).backbone(T.randn(1, 3, 448, 448))\n",
    "    last_3_shapes = [backbone_out[a].shape for a in list(backbone_out)[-3:]]\n",
    "    print(f\"model: {model}: {last_3_shapes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 256, 7, 7]), torch.Size([1, 256, 7, 7]), torch.Size([1, 256, 4, 4])]\n"
     ]
    }
   ],
   "source": [
    "uca = mocapModels.UpsampleCrossAttentionNetwork(\n",
    "    backbone=object_detection_models[\"fasterrcnn_mobilenet_v3_large_320_fpn\"].backbone,\n",
    "    neck=mocapModels.UpsampleCrossAttentionrNeck(128, 256, 1),\n",
    "    heads={\n",
    "        \"bboxes\": mocapModels.SelfAttentionHead(128, 4, 10, 256)\n",
    "    }\n",
    ")\n",
    "\n",
    "backbone_out = uca.backbone(T.randn(1, 3, 224, 224))\n",
    "print([v.shape for v in list(backbone_out.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae430eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_models = {\n",
    "    \"fcn_resnet50\": models.segmentation.fcn_resnet50(weights=models.segmentation.FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"fcn_resnet101\": models.segmentation.fcn_resnet101(weights=models.segmentation.FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_resnet50\": models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_resnet101\": models.segmentation.deeplabv3_resnet101(weights=models.segmentation.DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_mobilenet_v3_large\": models.segmentation.deeplabv3_mobilenet_v3_large(weights=models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"lraspp_mobilenet_v3_large\": models.segmentation.lraspp_mobilenet_v3_large(weights=models.segmentation.LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "}\n",
    "\n",
    "instance_segmentation_models = {\n",
    "    \"maskrcnn_resnet50_fpn\": models.detection.maskrcnn_resnet50_fpn(weights=models.detection.MaskRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"maskrcnn_resnet50_fpn_v2\": models.detection.maskrcnn_resnet50_fpn_v2(weights=models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "}\n",
    "\n",
    "keypoint_detection_models = {\n",
    "    \"keypointrcnn_resnet50_fpn\": models.detection.keypointrcnn_resnet50_fpn(weights=models.detection.KeypointRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15922c-cfdb-4296-be6d-8fee39057e1e",
   "metadata": {},
   "source": [
    "## Facer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd38233-a520-45e1-841c-c2fab5d2561d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
