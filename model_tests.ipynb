{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa25ff3-8adf-4c7a-a9b0-c66b25a7c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deformable_attention import DeformableAttention\n",
    "\n",
    "from model.endtoendmodels import SpatialEncoder\n",
    "from model.benchmarking import model_speedtest\n",
    "from core.torchhelpers import positional_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e447f378-c615-4835-a144-08bf916c4773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        self.LN1 = nn.LayerNorm(normalized_shape=input_dim, eps=0.00001, elementwise_affine=True)\n",
    "        self.LN2 = nn.LayerNorm(normalized_shape=input_dim, eps=0.00001, elementwise_affine=True)\n",
    "        \n",
    "        self.MSA = nn.MultiheadAttention(\n",
    "            embed_dim=input_dim,\n",
    "            num_heads=1,\n",
    "            dropout=0,\n",
    "            bias=True,\n",
    "            add_bias_kv=False,\n",
    "            add_zero_attn=False,\n",
    "            batch_first=True)\n",
    "        \n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=(input_dim + output_dim) // 2, bias=True),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=(input_dim + output_dim) // 2, out_features=output_dim, bias=True),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.residual_connection = nn.Sequential(nn.Linear(input_dim, output_dim), nn.ELU())\n",
    "\n",
    "    def forward(self, x: T.Tensor):\n",
    "\n",
    "        residual = self.residual_connection(x)\n",
    "\n",
    "        x = self.LN1(x)\n",
    "        x, attention_matrix = self.MSA(x, x, x)\n",
    "        \n",
    "        x = self.MLP(self.LN2(x))\n",
    "\n",
    "        x = T.add(residual, x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# TransformerEncoderBlock(256, 128)(T.rand(1, 3, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f343d845-eea1-4c44-9bf6-43b98aaba360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 bias: bool = False,\n",
    "                 dilation: int = 1,\n",
    "                 padding: int = 0,\n",
    "                 groups: int = 1\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, \n",
    "            groups=groups, bias=bias, padding_mode = \"zeros\")\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x: T.Tensor):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 shortcut: bool = True\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, (in_channels + out_channels) // 2, kernel_size, padding=1)\n",
    "        self.conv2 = ConvBlock((in_channels + out_channels) // 2, out_channels, kernel_size, padding=1)\n",
    "\n",
    "        self.add = shortcut and in_channels == out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.conv1(x)) if self.add else self.conv2(self.conv1(x))\n",
    "        \n",
    "class C2f(nn.Module):\n",
    "    # CSP Bottleneck with 2 convolutions\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 n: int = 1,\n",
    "                 shortcut: bool = True\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        self.hidden_channels = (in_channels + out_channels) // 2\n",
    "\n",
    "        self.conv1 = ConvBlock(in_channels, self.hidden_channels * 2, kernel_size=kernel_size, stride=1)\n",
    "        \n",
    "        self.module_list = nn.ModuleList(BottleneckBlock(\n",
    "            in_channels=self.hidden_channels, \n",
    "            out_channels=self.hidden_channels, \n",
    "            kernel_size=3, \n",
    "            shortcut=shortcut) for _ in range(n))\n",
    "\n",
    "        self.conv2 = ConvBlock((2 + n) * self.hidden_channels, out_channels, kernel_size=kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.conv1(x).split((self.hidden_channels, self.hidden_channels), 1))\n",
    "        y.extend(module(y[-1]) for module in self.module_list)\n",
    "        return self.conv2(T.cat(y, 1))\n",
    "\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 maxpool_kernel_size=5 # equivalent to SPP(k=(5, 9, 13))\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, in_channels, kernel_size=1, stride=1)\n",
    "        self.conv2 = ConvBlock(in_channels * 4, out_channels, kernel_size=1, stride=1)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=1, padding=maxpool_kernel_size//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x1 = self.max_pool(x)\n",
    "        x2 = self.max_pool(x1)\n",
    "        x3 = self.max_pool(x2)\n",
    "        return self.conv2(T.cat([x, x1, x2, x3], 1))\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, \n",
    "                 depth_multiple = 0.33,\n",
    "                 width_multiple = 0.25\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        scaled_64 = int(64 * width_multiple)\n",
    "        scaled_128 = int(128 * width_multiple)\n",
    "        scaled_256 = int(256 * width_multiple)\n",
    "        scaled_512 = int(512 * width_multiple)\n",
    "        scaled_1024 = int(1024 * width_multiple)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            ConvBlock(3, scaled_64, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(scaled_64, scaled_128, kernel_size=3, stride=2, padding=1),\n",
    "            C2f(scaled_128, scaled_128, kernel_size=1, n=int(3*depth_multiple), shortcut=True),\n",
    "            ConvBlock(scaled_128, scaled_256, kernel_size=3, stride=2, padding=1),\n",
    "            C2f(scaled_256, scaled_256, kernel_size=1, n=int(6*depth_multiple), shortcut=True)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            ConvBlock(scaled_256, scaled_512, kernel_size=3, stride=2, padding=1),\n",
    "            C2f(scaled_512, scaled_512, kernel_size=1, n=int(6*depth_multiple), shortcut=True),\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            ConvBlock(scaled_512, scaled_1024, kernel_size=3, stride=2, padding=1),\n",
    "            C2f(scaled_1024, scaled_1024, kernel_size=1, n=int(3*depth_multiple), shortcut=True),\n",
    "            SPPF(scaled_1024, scaled_1024)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> tuple[T.Tensor, T.Tensor, T.Tensor]:\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "\n",
    "        return x1, x2, x3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3f892964-2e49-49aa-8ef0-5a9799f41df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UpsampleTransformerNeck(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_size,\n",
    "                 depth_multiple = 0.33,\n",
    "                 width_multiple = 0.25,\n",
    "                 original_positional_embedding_size: int = 256\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        scaled_256 = int(256 * width_multiple)\n",
    "        scaled_512 = int(512 * width_multiple)\n",
    "        scaled_1024 = int(1024 * width_multiple)\n",
    "        scaled_positional_embedding_size = int(original_positional_embedding_size * width_multiple)\n",
    "        \n",
    "        self.upsample_x2 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        \n",
    "        self.reverse1 = nn.Sequential(\n",
    "            C2f(scaled_1024 + scaled_512, scaled_512, kernel_size=1, n=int(3*depth_multiple), shortcut=False)\n",
    "        )\n",
    "        self.reverse2 = nn.Sequential(\n",
    "            C2f(scaled_512 + scaled_256, scaled_512, kernel_size=1, n=int(3*depth_multiple), shortcut=False),\n",
    "            ConvBlock(scaled_512, scaled_512, kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.reverse3 = nn.Sequential(\n",
    "            C2f(scaled_512 * 2, scaled_512, kernel_size=1, n=int(3*depth_multiple), shortcut=False),\n",
    "            ConvBlock(scaled_512, scaled_1024, kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.Q_encoder = ConvBlock(scaled_1024, scaled_1024, kernel_size=1, stride=1, padding=0)\n",
    "        self.K_encoder = ConvBlock(scaled_1024, scaled_1024, kernel_size=1, stride=1, padding=0)\n",
    "        self.V_encoder = nn.Sequential(\n",
    "            # ConvBlock(scaled_1024, scaled_1024, kernel_size=1, stride=1, padding=0),\n",
    "            # C2f(scaled_1024, scaled_1024, kernel_size=1, n=int(3*depth_multiple), shortcut=True),\n",
    "            # ConvBlock(scaled_1024, scaled_1024, kernel_size=1, stride=1, padding=0),\n",
    "            # C2f(scaled_1024, scaled_1024, kernel_size=1, n=int(3*depth_multiple), shortcut=True),\n",
    "            ConvBlock(scaled_1024, scaled_1024 + scaled_positional_embedding_size, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(positional_embedding(20*20, scaled_positional_embedding_size), requires_grad=False)\n",
    "\n",
    "        self.output_1d_conv = nn.Conv1d(scaled_1024 + scaled_positional_embedding_size, output_size, kernel_size=1, stride=1, padding=0, groups=1)\n",
    "\n",
    "    \n",
    "    def forward(self, x1: T.Tensor, x2: T.Tensor, x3: T.Tensor) -> T.Tensor: \n",
    "        # x1, x2, x3 in order: middle of the backbone to final layer output\n",
    "\n",
    "        y1 = T.cat([self.upsample_x2(x3), x2], 1)\n",
    "        y1 = self.reverse1(y1)\n",
    "\n",
    "        y2 = T.cat([self.upsample_x2(y1), x1], 1)\n",
    "        y2 = self.reverse2(y2)\n",
    "\n",
    "        y3 = T.cat([y1, y2], 1)\n",
    "        y3 = self.reverse3(y3)\n",
    "\n",
    "        Q = self.Q_encoder(y3).flatten(2).permute(0, 2, 1)\n",
    "        Q = T.cat([Q, self.positional_embedding[:Q.shape[1]].expand(Q.shape[0], -1, -1)], 2)\n",
    "\n",
    "        K = self.K_encoder(y3).flatten(2).permute(0, 2, 1)\n",
    "        K = T.cat([K, self.positional_embedding[:K.shape[1]].expand(K.shape[0], -1, -1)], 2)\n",
    "\n",
    "        V = self.V_encoder(x3).flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        out = nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "        out = self.output_1d_conv(out.permute(0, 2, 1))\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "class SimpleTransformerHead(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 output_length,\n",
    "                 width_multiple = 0.25,\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        scaled_256 = int(256 * width_multiple)\n",
    "        scaled_512 = int(512 * width_multiple)\n",
    "        scaled_1024 = int(1024 * width_multiple)\n",
    "\n",
    "        self.output_length = output_length\n",
    "\n",
    "        self.input_1d_conv = nn.Sequential(\n",
    "            nn.Conv1d(input_size, scaled_1024, kernel_size=1, stride=1, padding=0, groups=1),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(scaled_1024)\n",
    "        )\n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(positional_embedding(20*20, scaled_1024), requires_grad=False)\n",
    "\n",
    "        self.self_attention = TransformerEncoderBlock(scaled_1024, output_size)\n",
    "        \n",
    "        # self.forward_encoder = nn.Sequential(\n",
    "        #     nn.Linear(scaled_1024, scaled_1024),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.BatchNorm1d(scaled_1024),\n",
    "        #     nn.Linear(scaled_1024, scaled_1024),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.BatchNorm1d(scaled_1024)\n",
    "        # )\n",
    "        self.internal_state = nn.Parameter(T.rand(output_length, scaled_1024, dtype=T.float32), requires_grad=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x: T.Tensor, corrections: int = 3) -> T.Tensor:\n",
    "\n",
    "        x = self.input_1d_conv(x).permute(0, 2, 1)\n",
    "\n",
    "        x = T.cat([x, self.internal_state.expand(x.shape[0], -1, -1)], 1)\n",
    "\n",
    "        x = self.self_attention(x)\n",
    "\n",
    "        return x[:, -self.output_length:, :]\n",
    "\n",
    "\n",
    "class CodebookTransformerHead(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 width_multiple = 0.25,\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "class CascadedTransformerHead(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 width_multiple = 0.25,\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        scaled_256 = int(256 * width_multiple)\n",
    "        scaled_512 = int(512 * width_multiple)\n",
    "        scaled_1024 = int(1024 * width_multiple)\n",
    "\n",
    "        self.input_1d_conv = nn.Conv1d(input_size, scaled_1024, kernel_size=1, stride=1, padding=0, groups=1)\n",
    "        self.input_1d_conv_memory = nn.Conv1d(input_size, scaled_1024, kernel_size=1, stride=1, padding=0, groups=1)\n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(positional_embedding(20*20, scaled_1024), requires_grad=False)\n",
    "\n",
    "        self.forward_encoder = nn.Sequential(\n",
    "            nn.Linear(scaled_1024, scaled_1024),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(scaled_1024),\n",
    "            nn.Linear(scaled_1024, scaled_1024),\n",
    "            nn.SiLU(),\n",
    "            nn.BatchNorm1d(scaled_1024)\n",
    "        )\n",
    "\n",
    "        self.R = nn.Parameter(T.rand(output_size, dtype=T.float32), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x: T.Tensor, corrections: int = 3) -> T.Tensor:\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        M = self.input_1d_conv_memory(x).permute(0, 2, 1)\n",
    "        R = self.R\n",
    "        X = self.input_1d_conv(x).permute(0, 2, 1)\n",
    "\n",
    "        for _ in range(corrections):\n",
    "\n",
    "            q = X + self.positional_embedding.expand(X.shape[0], -1, -1)\n",
    "            k = X + self.positional_embedding.expand(X.shape[0], -1, -1)\n",
    "            v = X\n",
    "\n",
    "            Q = X + nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "            print(Q.shape, M.shape, R.shape)\n",
    "            \n",
    "            y = nn.functional.scaled_dot_product_attention(Q, M, R)\n",
    "            y = y + Q\n",
    "            y = self.forward_encoder(y)\n",
    "\n",
    "            print(y.shape)\n",
    "\n",
    "        \n",
    "        return out\n",
    "        \n",
    "class DeformableAttentionHead(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 width_multiple = 0.25,\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        scaled_256 = int(256 * width_multiple)\n",
    "        scaled_512 = int(512 * width_multiple)\n",
    "        scaled_1024 = int(1024 * width_multiple)\n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(positional_embedding(20*20, input_size), requires_grad=False)\n",
    "\n",
    "        self.deformable_attn = DeformableAttention(\n",
    "            dim = input_size,\n",
    "            dim_head = input_size // 8,               # dimension per head\n",
    "            heads = 8,\n",
    "            dropout = 0.,\n",
    "            downsample_factor = 4,       # downsample factor (r in paper)\n",
    "            offset_scale = 4,            # scale of offset, maximum offset\n",
    "            offset_groups = None,        # number of offset groups, should be multiple of heads\n",
    "            offset_kernel_size = 6,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        raise NotImplementedError()\n",
    "        x = x.reshape(x.shape[0], x.shape[1], int(math.sqrt(x.shape[2])), int(math.sqrt(x.shape[2])))\n",
    "        out = self.deformable_attn(x)\n",
    "        return out\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 neck_output_size,\n",
    "                 head_output_size,\n",
    "                 head_output_length,\n",
    "                 depth_multiple = 0.33,\n",
    "                 width_multiple = 0.25\n",
    "                ):\n",
    "        super(type(self), self).__init__()\n",
    "\n",
    "        self.backbone = Backbone(depth_multiple, width_multiple)\n",
    "        self.neck = UpsampleTransformerNeck(neck_output_size, depth_multiple, width_multiple)\n",
    "        \n",
    "        self.head = SimpleTransformerHead(neck_output_size, head_output_size, head_output_length, depth_multiple)\n",
    "        \n",
    "        print(f\"running with neck: {self.neck.__class__}\")\n",
    "\n",
    "    def forward(self, x: T.Tensor):\n",
    "        \n",
    "        neck_out = self.neck(*self.backbone(x))\n",
    "        \n",
    "        return self.head(neck_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d43ff56b-f14d-442e-8d2b-42a65f5ae0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with neck: <class '__main__.UpsampleTransformerNeck'>\n",
      "torch.Size([1, 98, 3])\n",
      "model size: 3.383514\n",
      "fps: 218.67487831943072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = T.rand(1, 3, 640, 640).to(\"cuda\")\n",
    "net = FullModel(512, 3, 98, 0.25).to(\"cuda\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3ddca-e1cb-4d38-9db3-59bcde2f4598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98450b30-8b79-4cc5-ba6d-ad5bfde103b9",
   "metadata": {},
   "source": [
    "## Interesting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4d58cd-505d-49bc-ba36-909dc91d0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regnet_x_400mf\n",
      "speedtest engaged ...\n",
      "number of parameters: 5.495975971221924 M\n",
      "test completed\n",
      "fps: 121.81525505151455\n",
      "\n",
      "regnet_x_800mf\n",
      "speedtest engaged ...\n",
      "number of parameters: 7.259655952453613 M\n",
      "test completed\n",
      "fps: 95.65943772882376\n",
      "\n",
      "regnet_y_400mf\n",
      "speedtest engaged ...\n",
      "number of parameters: 4.344143867492676 M\n",
      "test completed\n",
      "fps: 97.7023625974944\n",
      "\n",
      "regnet_y_800mf\n",
      "speedtest engaged ...\n",
      "number of parameters: 6.432511806488037 M\n",
      "test completed\n",
      "fps: 92.50598632228807\n",
      "\n",
      "vit_b_32\n",
      "speedtest engaged ...\n",
      "number of parameters: 88.22423553466797 M\n",
      "test completed\n",
      "fps: 63.93281645726603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_interesting_models = {\n",
    "    \"regnet_x_400mf\": models.get_model(\"regnet_x_400mf\").to(\"cuda\"),\n",
    "    \"regnet_x_800mf\": models.get_model(\"regnet_x_800mf\").to(\"cuda\"),\n",
    "    \"regnet_y_400mf\": models.get_model(\"regnet_y_400mf\").to(\"cuda\"),\n",
    "    \"regnet_y_800mf\": models.get_model(\"regnet_y_800mf\").to(\"cuda\"),\n",
    "    \"vit_b_32\": models.get_model(\"vit_b_32\").to(\"cuda\"),\n",
    "}\n",
    "\n",
    "for model_name in most_interesting_models:\n",
    "    print(model_name)\n",
    "    model_speedtest(most_interesting_models[model_name], (10, 3, 224, 224))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b91422-af1d-4cc8-8d69-3f90e9bd4531",
   "metadata": {},
   "source": [
    "## PyTorch Pretrained Models Speedtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01c00a-fa1e-4619-a138-af1280b37408",
   "metadata": {},
   "source": [
    "### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "683b4609-fcc4-4e87-a2b7-e489223c52e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_b_16\n",
      "speedtest engaged ...\n",
      "number of parameters: 86.56765747070312 M\n",
      "test completed\n",
      "fps: 112.87943783821194\n",
      "\n",
      "vit_b_32\n",
      "speedtest engaged ...\n",
      "number of parameters: 88.22423553466797 M\n",
      "test completed\n",
      "fps: 226.80375376224916\n",
      "\n",
      "vit_h_14\n",
      "speedtest engaged ...\n",
      "number of parameters: 632.0458374023438 M\n",
      "test completed\n",
      "fps: 14.448708363441009\n",
      "\n",
      "vit_l_16\n",
      "speedtest engaged ...\n",
      "number of parameters: 304.3266296386719 M\n",
      "test completed\n",
      "fps: 35.05876075562481\n",
      "\n",
      "vit_l_32\n",
      "speedtest engaged ...\n",
      "number of parameters: 306.535400390625 M\n",
      "test completed\n",
      "fps: 105.79344269428543\n",
      "\n",
      "wide_resnet101_2\n",
      "speedtest engaged ...\n",
      "number of parameters: 126.8866958618164 M\n",
      "test completed\n",
      "fps: 50.2993246548358\n",
      "\n",
      "wide_resnet50_2\n",
      "speedtest engaged ...\n",
      "number of parameters: 68.88323974609375 M\n",
      "test completed\n",
      "fps: 101.39064082574481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classiciation_models = models.list_models()\n",
    "\n",
    "for model_name in classiciation_models:\n",
    "    print(model_name)\n",
    "    model_speedtest(models.get_model(model_name).to(\"cuda\"), (1, 3, 224, 224))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ed89f-cdae-4cd7-bfe7-84c55a9eee29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3528660-0997-40b2-9107-0b1b9b0bb972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn_resnet50\n",
      "speedtest engaged ...\n",
      "number of parameters: 35.32221603393555 M\n",
      "test completed\n",
      "fps: 30.42503202261378\n",
      "\n",
      "fcn_resnet101\n",
      "speedtest engaged ...\n",
      "number of parameters: 54.3143424987793 M\n",
      "test completed\n",
      "fps: 18.950708659613525\n",
      "\n",
      "deeplabv3_resnet50\n",
      "speedtest engaged ...\n",
      "number of parameters: 42.00407028198242 M\n",
      "test completed\n",
      "fps: 21.58508996294815\n",
      "\n",
      "deeplabv3_resnet101\n",
      "speedtest engaged ...\n",
      "number of parameters: 60.99620056152344 M\n",
      "test completed\n",
      "fps: 13.971454170288638\n",
      "\n",
      "deeplabv3_mobilenet_v3_large\n",
      "speedtest engaged ...\n",
      "number of parameters: 11.029328346252441 M\n",
      "test completed\n",
      "fps: 106.99429738664644\n",
      "\n",
      "lraspp_mobilenet_v3_large\n",
      "speedtest engaged ...\n",
      "number of parameters: 3.2215380668640137 M\n",
      "test completed\n",
      "fps: 173.46716906285866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "segmentation_models = {\n",
    "    \"fcn_resnet50\": models.segmentation.fcn_resnet50(weights=models.segmentation.FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"fcn_resnet101\": models.segmentation.fcn_resnet101(weights=models.segmentation.FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_resnet50\": models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_resnet101\": models.segmentation.deeplabv3_resnet101(weights=models.segmentation.DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"deeplabv3_mobilenet_v3_large\": models.segmentation.deeplabv3_mobilenet_v3_large(weights=models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "    \"lraspp_mobilenet_v3_large\": models.segmentation.lraspp_mobilenet_v3_large(weights=models.segmentation.LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1),\n",
    "}\n",
    "\n",
    "for model_name in segmentation_models:\n",
    "    print(model_name)\n",
    "    model_speedtest(segmentation_models[model_name].to(\"cuda\"), (1, 3, 448, 448))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07a9b7-255f-4902-8b68-19695b12bfc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a4a5c3-4861-4a0a-b9d1-563019a548bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_models = {\n",
    "    \"fcos_resnet50_fpn\": models.detection.fcos_resnet50_fpn(weights=models.detection.FCOS_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_320_fpn\": models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_mobilenet_v3_large_fpn\": models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn_v2\": models.detection.fasterrcnn_resnet50_fpn_v2(weights=models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"fasterrcnn_resnet50_fpn\": models.detection.fasterrcnn_resnet50_fpn(weights=models.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn_v2\": models.detection.retinanet_resnet50_fpn_v2(weights=models.detection.RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "    \"retinanet_resnet50_fpn\": models.detection.retinanet_resnet50_fpn(weights=models.detection.RetinaNet_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"ssd300_vgg16\": models.detection.ssd300_vgg16(weights=models.detection.SSD300_VGG16_Weights.COCO_V1),\n",
    "    \"ssdlite320_mobilenet_v3_large\": models.detection.ssdlite320_mobilenet_v3_large(weights=models.detection.SSDLite320_MobileNet_V3_Large_Weights.COCO_V1),\n",
    "}\n",
    "for model_name in object_detection_models:\n",
    "    print(model_name)\n",
    "    model_speedtest(object_detection_models[model_name].to(\"cuda\"), (1, 3, 448, 448))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df5159-d32f-4243-8e0a-b39879350dab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Instance Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6ac0f0-fc38-49c9-901e-a5a93b77ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maskrcnn_resnet50_fpn\n",
      "speedtest engaged ...\n",
      "number of parameters: 44.40139389038086 M\n",
      "test completed\n",
      "fps: 17.763518343754907\n",
      "\n",
      "maskrcnn_resnet50_fpn_v2\n",
      "speedtest engaged ...\n",
      "number of parameters: 46.35940933227539 M\n",
      "test completed\n",
      "fps: 9.48244126087252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instance_segmentation_models = {\n",
    "    \"maskrcnn_resnet50_fpn\": models.detection.maskrcnn_resnet50_fpn(weights=models.detection.MaskRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "    \"maskrcnn_resnet50_fpn_v2\": models.detection.maskrcnn_resnet50_fpn_v2(weights=models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1),\n",
    "}\n",
    "for model_name in instance_segmentation_models:\n",
    "    print(model_name)\n",
    "    model_speedtest(instance_segmentation_models[model_name].to(\"cuda\"), (1, 3, 448, 448))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b6ec0-c024-4664-af32-79067ab06501",
   "metadata": {},
   "source": [
    "### Keypoint detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417195ea-e7cb-467a-b6a9-8fc841e4764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_detection_models = {\n",
    "    \"keypointrcnn_resnet50_fpn\": models.detection.keypointrcnn_resnet50_fpn(weights=models.detection.KeypointRCNN_ResNet50_FPN_Weights.COCO_V1),\n",
    "}\n",
    "for model_name in keypoint_detection_models:\n",
    "    print(model_name)\n",
    "    model_speedtest(keypoint_detection_models[model_name].to(\"cuda\"), (1, 3, 224, 224))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cac9f3-e062-43ca-aefc-85933471c866",
   "metadata": {},
   "source": [
    "## Ultralytics YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be67f34e-be6e-4d0b-8482-2d6f2dcc4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb3c8333-0ef1-4a96-b871-7c4710bbf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov8_model = YOLO(\"yolov8n-pose.pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af212c8-8c2a-4240-a56d-d558bede58f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 68.0ms\n",
      "Speed: 0.0ms preprocess, 68.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 19.0ms\n",
      "Speed: 0.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 15.0ms\n",
      "Speed: 0.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 15.0ms\n",
      "Speed: 0.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 14.0ms\n",
      "Speed: 0.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 14.0ms\n",
      "Speed: 0.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 14.0ms\n",
      "Speed: 0.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.8ms\n",
      "Speed: 0.0ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.2ms\n",
      "Speed: 0.0ms preprocess, 7.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.6ms\n",
      "Speed: 0.0ms preprocess, 6.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.1ms\n",
      "Speed: 0.0ms preprocess, 6.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.9ms\n",
      "Speed: 0.0ms preprocess, 6.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m frame \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m \u001b[43myolov8_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m T\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m     11\u001b[0m time_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m dt)\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\engine\\model.py:262\u001b[0m, in \u001b[0;36mModel.track\u001b[1;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# ByteTrack-based method needs low confidence predictions as input\u001b[39;00m\n\u001b[0;32m    261\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\engine\\model.py:242\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:196\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:264\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds, im, im0s)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mon_predict_postprocess_end\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n\u001b[0;32m    267\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(im0s)\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:358\u001b[0m, in \u001b[0;36mBasePredictor.run_callbacks\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs all registered callbacks for a specific event.\"\"\"\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mget(event, []):\n\u001b[1;32m--> 358\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\trackers\\track.py:48\u001b[0m, in \u001b[0;36mon_predict_postprocess_end\u001b[1;34m(predictor)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(det) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrackers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tracks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\trackers\\byte_tracker.py:278\u001b[0m, in \u001b[0;36mBYTETracker.update\u001b[1;34m(self, results, img)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_predict(strack_pool)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmc\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     warp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     STrack\u001b[38;5;241m.\u001b[39mmulti_gmc(strack_pool, warp)\n\u001b[0;32m    280\u001b[0m     STrack\u001b[38;5;241m.\u001b[39mmulti_gmc(unconfirmed, warp)\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\trackers\\utils\\gmc.py:85\u001b[0m, in \u001b[0;36mGMC.apply\u001b[1;34m(self, raw_frame, detections)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplyEcc(raw_frame, detections)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparseOptFlow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplySparseOptFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\programming\\video-to-animation\\.venv\\lib\\site-packages\\ultralytics\\trackers\\utils\\gmc.py:250\u001b[0m, in \u001b[0;36mGMC.applySparseOptFlow\u001b[1;34m(self, raw_frame, detections)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize.\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m height, width, _ \u001b[38;5;241m=\u001b[39m raw_frame\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 250\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m H \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Downscale image\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "time_ = 0\n",
    "\n",
    "for _ in range(500):\n",
    "    frame = T.rand(1, 3, 640, 640).to(\"cuda\")\n",
    "\n",
    "    dt = time.time()\n",
    "    \n",
    "    yolov8_model.track(frame, persist=True)\n",
    "    T.cuda.synchronize()\n",
    "\n",
    "    time_ += (time.time() - dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830b0f28-9024-495a-9ecb-a6ad3509c88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233.60752658527196"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (time_ / 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15922c-cfdb-4296-be6d-8fee39057e1e",
   "metadata": {},
   "source": [
    "## Facer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00cc738-7583-46fa-b879-7459df116164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import facer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0c0ab11-bb8d-4864-bdab-64ad4f33a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "facer_models = {\n",
    "    # \"retinaface/mobilenet\": facer.face_detector(\"retinaface/mobilenet\", device=\"cuda\"),\n",
    "    # \"retinaface/resnet\": facer.face_detector(\"retinaface/resnet50\", device=\"cuda\"),\n",
    "    \"farl/lapa/448\": facer.face_parser(\"farl/lapa/448\", device=\"cuda\"),\n",
    "    \"farl/celebm/448\": facer.face_parser(\"farl/celebm/448\", device=\"cuda\"),\n",
    "    \"farl/ibug300w/448\": facer.face_aligner(\"farl/ibug300w/448\", device=\"cuda\"),\n",
    "    \"farl/aflw19/448\": facer.face_aligner(\"farl/aflw19/448\", device=\"cuda\"),\n",
    "    \"farl/wflw/448\": facer.face_aligner(\"farl/wflw/448\", device=\"cuda\"),\n",
    "    \"farl/celeba/224\": facer.face_attr(\"farl/celeba/224\", device=\"cuda\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd38233-a520-45e1-841c-c2fab5d2561d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
